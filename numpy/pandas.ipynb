{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ------- INSTALL LIBRARIES  -------- #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTALL THE PANDAS LIBRARY\n",
    "!pip3 install --upgrade pandas\n",
    "!pip3 install --upgrade numpy\n",
    "!pip3 install --upgrade openpyxl\n",
    "!pip3 install --upgrade pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ---------- About Pandas ----------- #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is Pandas...\n",
    "\"\"\" \n",
    "Pandas is the open source python liberary which provide high-performance, easy-to-use data stractures and data analytis tool.\n",
    "And it is useful to handle different types of data and perform the operation on these data. \n",
    "\"\"\"\n",
    "\n",
    "# Features of Pandas is... \n",
    "\"\"\"\n",
    "* It's well suited for tabular data as in sql/excel, ordered/unordered series data and arbitrary matrix data.\n",
    "* It's use for huge amount of data processing and analysis of relation data.\n",
    "* It have two primary data structure such as Series, Dataframe for handle vast varity of use cases in finance, statistic and many more.\n",
    "* pandas is built on top of NumPy and is intended to integrate well within a scientific computing environment with many other\n",
    "  3rd party libraries.\n",
    "\"\"\"\n",
    "\n",
    "# About the Series and Dataframe...\n",
    "\"\"\"\n",
    "1. Series : Series is a 1-Dimensional array which can store one type values as list.\n",
    "\n",
    "2. DataFrame : DataFrame is a 2-Dimensional array or matrix. Which is the combination of multiple series that can store\n",
    "              tabluar data where records can be use of rows and columns.\n",
    "\"\"\"\n",
    "\n",
    "# Here are just a few of the things that pandas does well...\n",
    "\"\"\"\n",
    "* Easy handling of missing data (represented as NaN) in floating point as well as non-floating point data\n",
    "* Size mutability: columns can be inserted and deleted from DataFrame and higher dimensional objects\n",
    "* Automatic and explicit data alignment: objects can be explicitly aligned to a set of labels, or the user can simply\n",
    "  ignore the labels and let Series, DataFrame, etc. automatically align the data for you in computations\n",
    "* Powerful, flexible group by functionality to perform split-apply-combine operations on data sets, for both aggregating\n",
    "  and transforming data\n",
    "* Make it easy to convert ragged, differently-indexed data in other Python and NumPy data structures into DataFrame objects\n",
    "* Intelligent label-based slicing, fancy indexing, and subsetting of large data sets\n",
    "* Intuitive merging and joining data sets\n",
    "* Flexible reshaping and pivoting of data sets\n",
    "* Hierarchical labeling of axes (possible to have multiple labels per tick)\n",
    "* Robust IO tools for loading data from flat files (CSV and delimited), Excel files, databases, and saving / loading data\n",
    "  from the ultrafast HDF5 format\n",
    "* Time series-specific functionality: date range generation and frequency conversion, moving window statistics,\n",
    "  date shifting, and lagging. \n",
    "\"\"\"\n",
    "\n",
    "# Mutability and copying of data...\n",
    "\"\"\"\n",
    "All pandas data structures are value-mutable (the values they contain can be altered) but not always size-mutable. The length of a Series\n",
    "cannot be changed, but, for example, columns can be inserted into a DataFrame. However, the vast majority of methods produce new objects\n",
    "and leave the input data untouched. In general we like to favor immutability where sensible.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ---------- Import Libraries ---------- ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "import openpyxl\n",
    "import json\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ----------- Dataframe And Series ------------- #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataframe..\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"Name\": [\n",
    "                \"Braund, Mr. Owen Harris\",\n",
    "                \"Allen, Mr. William Henry\",\n",
    "                \"Bonnell, Miss. Elizabeth\",\n",
    "            ],\n",
    "        \"Age\": [22, 35, 58],\n",
    "        \"Sex\": [\"male\", \"male\", \"female\"],\n",
    "    }\n",
    ")\n",
    "print(\"--------> Full DataFrame : \\n\", df, '\\n')\n",
    "# Show some of the rows from first or last..\n",
    "print(\"--------> First 2 rows : \\n\", df.head(2), '\\n')\n",
    "print(\"--------> Last 1 row : \\n\", df.tail(1))\n",
    "\n",
    "\n",
    "# Create the series..\n",
    "ages = pd.Series([12, 44, 25, 56], name=\"Age\")\n",
    "print(\"\\n---------> Series is : \\n\", ages)\n",
    "\n",
    "# Getting the series from existing dataframe\n",
    "# because each columns in dataframe is series...\n",
    "print(\"\\n---------> Get series from df : \\n\", df.Age)   # series can also get in df['Age'] format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ---------- READ AND WRITE DATA ----------- ##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"_READ AND WRITE THE TABULAR DATA IN DIFFERENT FILE FORMATS_\n",
    "\n",
    "REFERENCE : https://pandas.pydata.org/docs/user_guide/io.html\n",
    "\"\"\"\n",
    "\n",
    "# Use to_csv/read_csv to work with csv/txt file...\n",
    "# there are use path (location to read/write), sep (seperated by delimiter like \",/./-/\" etc), header (column names),\n",
    "# index (row numbers), index_col (to set columns as index) and so on..\n",
    "csv_df = pd.read_csv('data/csv_business_price.csv', low_memory=False, header=0, index_col=None)\n",
    "print(\"\\n--------> read csv data is : \\n\", csv_df.head(2))\n",
    "csv_df.to_csv('data/txt_business_price.txt', sep=',', index=False)\n",
    "\n",
    "\n",
    "# Read the text file using read_csv..\n",
    "txt_df = pd.read_csv('data/txt_business_price.txt', sep=',', low_memory=False)\n",
    "print(\"\\n--------> text data is :\\n\", txt_df.head(3))\n",
    "\n",
    "\n",
    "# Read the excel file using read_excel...\n",
    "excel_df = pd.read_excel('data/excel_business_operations.xlsx', header=0, index_col=0)\n",
    "print(\"\\n--------> excel data is :\\n\", excel_df.head(5))\n",
    "\n",
    "\n",
    "# Read the json file using read_json...\n",
    "# there can handling the different data structure at read the json file.\n",
    "# 1. Nested json : In this case, data file use data as nested objects in single object.\n",
    "#    which can be use by json_normalize() after loading the JSON data.\n",
    "# 2. Line delimited json : In this case, data file use data as multiple json objects each on new lines. \n",
    "#    which can be access by specifying \"line=True\"\n",
    "# Note : At the read/write json data, use the specific orientation for define the different format data structure. \n",
    "#        like : split, index, columns, records, table, values etc.\n",
    "# --------> Use Nested data. With define the max_level 0, 1 so on..\n",
    "with open('data/json_nested_data.json') as f:\n",
    "    data = json.load(f)\n",
    "json_nested_df = pd.json_normalize(data, max_level=1)\n",
    "print(\"\\n--------> json nested data is :\\n\", json_nested_df.head(10))\n",
    "# --------> Use Line delimited data.\n",
    "json_df = pd.read_json('data/json_line_delimited_flights.json', lines=True, orient=None)\n",
    "print(\"\\n--------> json line delimited data is :\\n\", json_df.head(5))\n",
    "\"\"\"\n",
    "Ref : https://pandas.pydata.org/docs/reference/api/pandas.read_json.html\n",
    "\"\"\"\n",
    "\n",
    "# Read the parquet file using read_parquet or by using pyarrow.parquet but need to import pyarrow module...\n",
    "parquet_df = pd.read_parquet('data/titanic.parquet', engine='auto') # ---> reading of parquet file can give issue on ipython/jupyter notebook.\n",
    "                                                                    #      so try on using python file.\n",
    "print(\"\\n----------> parquet file data is :\\n\", parquet_df.head(5))\n",
    "parquet_df.to_parquet('data/new_titanic.parquet')\n",
    "# read parquet using pyarrow library\n",
    "parq_df = pq.read_table('data/titanic.parquet').to_pandas()\n",
    "print(\"\\n-----------> read parquet data using pyarrow is :\\n\", parq_df.head(10))\n",
    "\n",
    "\n",
    "# Check the basic concepts..\n",
    "print(\"\\n---------> select the single column :\\n\", parq_df['Name'])\n",
    "print(\"\\n---------> select the multiple columns :\\n\", parq_df[['PassengerId', 'Pclass', 'Name', 'Sex', 'Age']])\n",
    "print(\"\\n--------> get the single field val : \\n\", parq_df['PassengerId'][0])        # get the field value..\n",
    "parq_df['PassengerId'][0] = 5\n",
    "print(\"\\n--------> change the field value : \\n\", parq_df.head(5))  # changes field value but it will gave the warning due to not good way..\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ---------- Uses The Dataframe And Series Functions/Attributes ------------ #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"_THIS IS ABOUT SOME COMMANLY USED PANDAS FUNCTIONS_\n",
    "\n",
    "   REFERENCE : https://pandas.pydata.org/docs/reference/frame.html\n",
    "\"\"\"\n",
    "\n",
    "# Use type function to indentify the Data structure...\n",
    "print(\"\\n-------> Identify the dataframe structure : \", type(df), \"\\n\")  \n",
    "print(\"\\n-------> Identify the series structure : \", type(ages), \"\\n\") \n",
    "\n",
    "# Use shape attribute to find out the count of rows and columns...\n",
    "print(\"\\n-------> Find the shape/size of the df is : \", df.shape, \"\\n\")     # ex-(3, 3) : means there are 3 rows and 3 columns..\n",
    "\n",
    "# Use the max/min function to Find maximum/minimum age from Dataframe/Series\n",
    "print(\"\\n--------> Max and Min age from df : \\n\", df.max(), '\\n\\n', df.min())\n",
    "print(\"\\n--------> Mix and Min age from series : \", ages.max(), ages.min())\n",
    "\n",
    "\n",
    "# Use describe to find out the statistic calculation at numerical data of dataframe..\n",
    "# or it is use for statistical analysis.\n",
    "print(\"\\n-------> Describe the statistic data is : \\n\", df.describe())\n",
    "\n",
    "# Use the info method to display all the info about the dataframe..\n",
    "print(\"\\n-------> Display the info of df is :\\n\")\n",
    "print(df.info())\n",
    "\n",
    "# Use dtypes to display the data type of dataframe or series...\n",
    "print(\"\\n-------> Types of the df : \\n\", df.dtypes)\n",
    "print(\"\\n-------> Types of the series : \\n\", ages.dtypes)\n",
    "\n",
    "# Use memory_usage function that showcase the memory usage of each columns..\n",
    "# and by specifying deep attribute as True, we can get actual space taken by each columns..\n",
    "print(\"\\n-------> Memory usage of the df columns : \\n\", df.memory_usage(deep=True))\n",
    "\n",
    "\n",
    "# Use index and columns for get the starting and ending position, columns name...\n",
    "print(\"\\n--------> Get df indexs :\\n\", df.index)\n",
    "df.index = ['first', 'second', 'third']\n",
    "print(\"\\n--------> change the index value : \\n\", df)                 # changes index value..\n",
    "print(\"\\n--------> Get df columns :\\n\", df.columns)\n",
    "\n",
    "# Use T for transpose the data frame from rows into columns and vise-versa...\n",
    "print(\"\\n--------> Transpose the df :\\n\", df.T)\n",
    "\n",
    "\n",
    "# Use astype for casting the data type of a specific column or all columns..\n",
    "temp_df = pd.DataFrame(data={'col1': [1, 2, 4, 6, 7, 3, 5 ,7], 'col2': [3, 4, 6, 8, 9, 5, 7, 9], 'col3': [23, 16, 27, 34, 98, 37, 22, 29]})\n",
    "print(\"\\n--------> Current datatype of temp data frame :\\n\", temp_df.dtypes)\n",
    "print(\"\\n--------> Cast single column is : \\n\", temp_df.astype({'col1': 'int32'}).dtypes)\n",
    "print(\"\\n--------> Cast all the columns are : \\n\", temp_df.astype('int32').dtypes)\n",
    "\n",
    "# Extract the sample dataframe from \"df\" and store it in \"sample_df\"\n",
    "df = pd.read_csv('data/csv_business_price.csv', low_memory=False)\n",
    "sample_df = df[['Series_reference', 'Period', 'STATUS', 'UNITS', 'Subject', 'Series_title_1', 'Series_title_2']].sample(15)\n",
    "print(\"\\n---------> Print the sample data frame :\\n\", sample_df)\n",
    "\n",
    "# Use sort index function for sort the dataframe based on index(rows)/columns...\n",
    "# Syntax: DataFrame.sort_index(axis=0, level=None, ascending=True, inplace=False, kind='quicksort',\n",
    "#                              na_position='last', sort_remaining=True, by=None)\n",
    "print(\"\\n--------> Sort the data index based on row axis :\\n\",\n",
    "        sample_df.sort_index(axis=0, ascending=False, inplace=False, kind='quicksort', na_position='first'))\n",
    "        # ex.- here axis=0 (row-wise), axis=1 (col-wise)\n",
    "\"\"\"More About : https://www.geeksforgeeks.org/python-pandas-dataframe-sort_index/ \"\"\" \n",
    "\n",
    "# Use sort values function for sort the dataframe based on values and row/column axis...\n",
    "# Syntax: DataFrame.sort_values(by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n",
    "print(\"\\n--------> Sort the data values based on row axis :\\n\", sample_df.sort_values(by=['Series_reference', 'Period'], axis=0, ascending=False, inplace=False, kind='quicksort', na_position='last'))\n",
    "\"\"\"More About : https://www.geeksforgeeks.org/python-pandas-dataframe-sort_values-set-1/ \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Use view(reference) or copy for data frame....\n",
    "df1 = temp_df                # df1 use as view/reference of temp_df bcz when will make change in df1 that also applied on actual temp_df.\n",
    "print(\"\\n---------> df1 use as view of temp_df : \\n\", df1)\n",
    "df1['col1'][0] = 6\n",
    "print(\"\\n---------> Actual temp_df after change df1 view : \\n\", temp_df)\n",
    "\n",
    "df1 = temp_df.copy()         # df1 use as copy of actual temp_df that will not make change temp_df after changes into df1.\n",
    "df1['col1'][0] = 3\n",
    "print(\"\\n---------> Actual temp_df after no change df1 copy : \\n\", temp_df)\n",
    "print(\"\\n---------> df1 use as copy of temp_df : \\n\", df1)\n",
    "\n",
    "\n",
    "\n",
    "# Whenever we have made direct changes in position value of view/copy dataframe. then in pandas, it can create issue to differenciat\n",
    "# in manipulation of view/copy df.  and that by it's raised the warning (same as in above case of update the value with 3).\n",
    "# So handle these all issues, Pandas allow us to use of loc and iloc functions..\n",
    "\n",
    "# There loc and iloc function are used for filtering, accessing and manupulating the data in database.\n",
    "# And the difference are loc (use with actual row and column name) while iloc (use of position number without care of row and columns name)\n",
    "# for menupulating records or extract the subset of dataframe...\n",
    "\"\"\" See in the documentation: \n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
    "\"\"\"\n",
    "# so basically loc/iloc used to take the copy after such operations.\n",
    "temp_df.loc[0, 'col1'] = 19                                           # use with actual row/col name\n",
    "print(\"\\n---------> Update value with loc : \\n\", temp_df)\n",
    "temp_df1 = temp_df.loc[[0, 1], ['col1', 'col3']]                       # get the subset table df from actual df. \n",
    "print(\"\\n---------> Get subset of df : \\n\", temp_df1)\n",
    "temp_df2 = temp_df.loc[:, ['col1', 'col3']]                          # get the subset table df with all row from actual df. \n",
    "print(\"\\n---------> Get subset of df with all rows : \\n\", temp_df2)\n",
    "temp_df3 = temp_df.loc[[0, 1], :]                                       # get the subset table df with all col from actual df. \n",
    "print(\"\\n---------> Get subset of df with all columns : \\n\", temp_df3)\n",
    "\n",
    "temp_df4 = temp_df.iloc[0, 0]                                     # use with position index\n",
    "print(\"\\n---------> Get value with iloc : \\n\", temp_df4)\n",
    "temp_df5 = temp_df.iloc[[0], :]                                   \n",
    "print(\"\\n---------> Get value with iloc : \\n\", temp_df5)\n",
    "\n",
    "\n",
    "# Use drop function to remove the unnacessary row/column...\n",
    "temp_df.loc[0, 0] = 123                                  # new col is create bcz column was not exiting.\n",
    "print(\"\\n---------> Df after Added/Update unnacessary col val : \\n\", temp_df)\n",
    "temp_df.drop([0], axis=1, inplace=True)                    # inplace=True use for changes applied on same dataframe.\n",
    "print(\"\\n---------> Df after Drop unnacessary col : \\n\", temp_df)\n",
    "\n",
    "\n",
    "# Use of reset_index function to reset the index value again in df..\n",
    "print(\"\\n---------> Df before reset : \\n\", temp_df)\n",
    "temp_df6 = temp_df.drop([3, 1], axis=0)\n",
    "print(\"\\n---------> Df before reset after drop : \\n\", temp_df6)\n",
    "temp_df6.reset_index(drop=True, inplace= True)\n",
    "print(\"\\n---------> Df after reset : \\n\", temp_df6)\n",
    "\n",
    "\n",
    "# Create new column with None or NaN values..\n",
    "# there are different way to add new column with none/nan values..\n",
    "temp_df7 = temp_df.copy()\n",
    "temp_df7['col4'] = None                  # assign no value col. to python oriented none, which is used for all data type in columns..\n",
    "print(\"\\n--------> Df col with none value :\\n\", temp_df7)   \n",
    "temp_df7['col5'] = np.nan              # assign no value col. to numpy NaN which is more appropriat for numarical column for data accuracy..\n",
    "print(\"\\n--------> Df col with np.nan value :\\n\", temp_df7)\n",
    "temp_df7.insert(4, 'col6', np.nan)      # assign no value col. using insert method with specific position..\n",
    "print(\"\\n--------> Df col with insert method is :\\n\", temp_df7)\n",
    "temp_df8 = temp_df7.assign(col7 = np.nan)   # assign no value col. using assign method which is more functional\n",
    "                                            # and return new df with effect original one..\n",
    "print(\"\\n-------> Df col with assign method without changes is :\\n\", temp_df7)\n",
    "print(\"\\n-------> Df col with assign method with changes is :\\n\", temp_df8)\n",
    "\n",
    "\n",
    "# Use of isNull and isNotNull function in df..\n",
    "# In pandas dataframe, isnull() and notnull() method are used to detect missing and non-missing values in dataframe..\n",
    "# And the purpose of identify the missing values that (represent as NaN for numerical columns, None for python object \n",
    "# and NaT for datetime)..\n",
    "# By using these method, return the boolean value True/False for missing and non-missing values in df...\n",
    "print(\"\\n---------> Df indicate missing or nan values is :\\n\", temp_df7.isna())     # here isna() and isnull() are same..\n",
    "print(\"\\n---------> Df indicate the missing or non values is :\\n\", temp_df7.isnull())\n",
    "print(\"\\n---------> Df indicate the non-missing value is :\\n\", temp_df7.notna())    # here notna() and notnull() are same..\n",
    "print(\"\\n---------> Df indicate the non-missing values is :\\n\", temp_df7.notnull())\n",
    "\n",
    "\n",
    "# Use of dropna function for remove the NaN value rows or update by default value inplace of NaN inside df...\n",
    "# syntax : DataFrame.dropna(*, axis=0, how=<no_default>, thresh=<no_default>, subset=None, inplace=False, ignore_index=False)\n",
    "# here..\n",
    "# axis=0 means remove row based on row index, axis=1 means remove column based on column index.\n",
    "# how=<value> define 'any' and 'all' at place of <value>\n",
    "#     Any - row/col removed if any values in row/col is NA, It is default\n",
    "#     All - row/col removed if all values in row/col are NA\n",
    "# thresh - it's define the number which is keep only the rows with at least n non-NA values.\n",
    "# subset - it's define in which columns to look for missing values.\n",
    "# ignore_index use for skip the row which will be mansion like [0, 3, 7 so on..]\n",
    "df = pd.DataFrame({\"name\": ['Alfred', 'Batman', 'Catwoman'],\n",
    "                   \"toy\": [np.nan, 'Batmobile', 'Bullwhip'],\n",
    "                   \"born\": [pd.NaT, pd.Timestamp(\"1940-04-25\"), pd.NaT]})\n",
    "print(\"\\n--------> drop all row with any NA value : \\n\", df.dropna())\n",
    "df.loc[:, 'sax'] = [np.nan, np.nan, np.nan]\n",
    "print(\"\\n--------> drop column with any NA value : \\n\", df.dropna(axis=1))\n",
    "print(\"\\n--------> drop row if all field in row conatins NA value : \\n\", df.dropna(how='all'))\n",
    "print(\"\\n--------> drop column if all field in col conatins NA value : \\n\", df.dropna(axis=1, how='all'))\n",
    "print(\"\\n--------> drop row and keep only at least 2 non-NA values rows : \\n\", df.dropna(thresh=2))\n",
    "print(\"\\n--------> based on given column check the NA value and drop if found NA : \\n\", df.dropna(subset=['name', 'toy']))\n",
    "\n",
    "\n",
    "# Use of drop_duplicates for remove the duplicate records from the df..\n",
    "# syntax : DataFrame.drop_duplicates(subset=None, *, keep='first', inplace=False, ignore_index=False)\n",
    "# here..\n",
    "# subset - only consider specify columns for indentify duplicate, default use of all the columns\n",
    "# keep - it's identify that duplicates should be contains. default is 'first' (use for keep first record from two or more duplicates)\n",
    "#        'last' (use for keep last record from duplicates), False (use for drop all duplicates)\n",
    "df = pd.DataFrame({\n",
    "    'brand': ['Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'],\n",
    "    'style': ['cup', 'cup', 'cup', 'pack', 'pack'],\n",
    "    'rating': [4, 4, 3.5, 15, 5]\n",
    "})\n",
    "print(\"\\n--------> drop duplicates based on all cols : \\n\", df.drop_duplicates())\n",
    "print(\"\\n--------> drop duplicates based on specified cols : \\n\", df.drop_duplicates(subset=['brand', 'style']))\n",
    "print(\"\\n--------> drop based on specify cols and keep last record : \\n\", df.drop_duplicates(subset=['brand', 'style'], keep='last'))\n",
    "\n",
    "\n",
    "# Use of fillna function for fill the NaN value by default value inside df..\n",
    "# syntax : DataFrame.fillna(value=None, *, method=None, axis=None, inplace=False, limit=None, downcast=<no_default>)\n",
    "# here..\n",
    "# value - use for fill the single value by default on index level, \n",
    "#         or use alternative for fill the nan/none value using dict/series/dataframe on column level.\n",
    "# method - it's use for fill based on ‘backfill’, ‘bfill’, ‘ffill’ and defaul is None.\n",
    "# axis - use axis=0 for index level fill or axis=1 for column level\n",
    "# limit - it's use for fill the nan up to by given number of limit.\n",
    "df = pd.read_excel('data/excel_business_operations.xlsx', header=0, index_col=0)\n",
    "df1 = df.iloc[0:10, :]\n",
    "df1.iloc[[1, 2, 8, 9], [0, 1]] = np.nan\n",
    "# It's fill NaN/None\n",
    "df1.columns = ['A', 'B']\n",
    "print(\"\\n--------> fill nan by default single val : \\n\", df1.fillna(0))       # by default, we can fill any value at index lavel\n",
    "print(\"\\n--------> fill nan with dict by each col val : \\n\",\n",
    "        df1.fillna({'A': \"UNKNOWN\", 'B': \"UNKNOWN\"}))    # dist fill values each column based.\n",
    "print(\"\\n--------> fill nan with dict val with up to limit no. : \\n\",\n",
    "        df1.fillna({'A': \"UNKNOWN\", 'B': \"UNKNOWN\"}, limit=3))   # it's fill values on given limit number of NaN/None\n",
    "t_df1 = pd.DataFrame(np.zeros((df1.shape[0]+1, df1.shape[1])), columns=df1.columns)\n",
    "print(\"\\n--------> fill nan with dataframe values : \\n\", df1.fillna(t_df1))   # fill the nan value by creating dataframe on column level\n",
    "\n",
    "\n",
    "# Use of isin() method in df..\n",
    "# syntax : DataFrame.isin(values)\n",
    "# values - It will return true if any the label match with values. and values is series, It should be match the all index with values.\n",
    "#           And values is dict, It should be match column index with values key name. In case of dataframe, It should match label\n",
    "#           and index both with the values.. \n",
    "df = pd.DataFrame({'num_legs': [2, 4], 'num_wings': [2, 0]},\n",
    "                  index=['falcon', 'dog'])\n",
    "\n",
    "print(\"\\n--------> isin use iterator val with col : \\n\", df['num_legs'].isin([2, 1])) # use as iterator value on single column.\n",
    "print(\"\\n--------> isin use iterator val with df : \\n\", df.isin([2, 0]))    # use as iterator value that match all the label\n",
    "print(\"\\n--------> isin use dict val with df : \\n\", df.isin({'num_wings': [1, 2]})) # use as dict value that match on specific column\n",
    "ser = pd.Series([2, 4], name='num_legs', index=['falcon', 'dogs'])\n",
    "print(\"\\n--------> isin use series val with df : \\n\", df.isin(ser)) # use as series values thet must match both index and column lebel\n",
    "print(\"\\n--------> isin use df val with df : \\n\", df.isin(pd.DataFrame({'num_legs': [2, 4], 'num_wings': [2, 0]},\n",
    "                  index=['falcon', 'dog']))) # use as df values that must also match both index and column label\n",
    "\n",
    "\n",
    "# Use of query() method in df..\n",
    "# It's use for Query the columns of dataframe with the boolean expression..\n",
    "# syntax : DataFrame.query(expr, *, inplace=False, **kwargs)\n",
    "# here..\n",
    "# expr (str) : The query string to evaluate.\n",
    "#            You can refer to variables in the environment by prefixing them with an ‘@’ character like @a + b.\n",
    "#            We can refer the column name, but if column contains some space or panctuation then use backtics like `num legs`.\n",
    "# ** kwargs : we can pass some keyword argument which can run through eval() or evaluation string.\n",
    "df = pd.DataFrame({'A': range(1, 6),\n",
    "                   'B': range(10, 0, -2),\n",
    "                   'C C': range(10, 5, -1)})\n",
    "print(\"\\n--------> query dataframe : \\n\", df)\n",
    "print(\"\\n--------> use query string on df : \\n\", df.query('A < B'))  # this is similar to query df[df.A < df.B]\n",
    "print(\"\\n--------> use query string on df space column : \\n\", df.query('B == `C C`'))  # this is similar to query df[df.B == df['C C']]\n",
    "\n",
    "\n",
    "# Use of str methods in df..\n",
    "# str accessor in pandas provides a collection of string methods that can be applied for manipulation string data \n",
    "# in series/index of a dataframe..\n",
    "# It's provides some variety of methods -\n",
    "#   Transformation: lower(), upper(), capitalize(), title(), strip(), replace(), etc.\n",
    "#   Searching and Matching: contains(), startswith(), endswith(), find(), match(), etc.\n",
    "#   Splitting and Joining: split(), join().\n",
    "#   Extraction: get(), extract().\n",
    "df = pd.DataFrame({'str_val' : ['lower', 'CAPITALS', 'this is a sentence', 'SwApCaSe', np.nan],\n",
    "                   'age' : [23, 45, 67, 20, 0]})\n",
    "print(\"\\n------> df : \\n\", df)\n",
    "ser = df.str_val\n",
    "print(\"------> ser in lower case : \\n\", ser.str.lower())     # convert string into lower case\n",
    "print(\"------> ser in upper case : \\n\", ser.str.upper())     # convert string into upper case\n",
    "print(\"------> ser in capitalized : \\n\", ser.str.capitalize()) # convert the first letter of string in upper case\n",
    "print(\"------> ser as title : \\n\", ser.str.title())     # return the string word start with capital letter\n",
    "print(\"------> ser is in swapcase : \\n\", ser.str.swapcase())  # use for convert lower string to upper case and vise-versa\n",
    "# concatinate the string using str.cat\n",
    "# syntax : Series.str.cat(others=None, sep=None, na_rep=None, join='left')\n",
    "# here others can be a series, dataframe or np.ndarray\n",
    "print(\"\\n------> cancat series : \\n\", ser.str.cat(sep=' ')) # When not passing others, all values are concatenated into a single string\n",
    "print(\"------> concat with seperation : \\n\", ser.str.cat(sep=' ', na_rep='?')) # nan values replace by ?\n",
    "print(\"------> concat with other series : \\n\", ser.str.cat(others=['case', 'letter', '!', 'letter', 'yes'], sep='_'))\n",
    "# centerlized the sting using center method\n",
    "# Series.str.center(width, fillchar=' ')\n",
    "print(\"\\n------> center series string : \\n\", ser.str.center(20, fillchar='.'))\n",
    "print(\"------> left justify string : \\n\", ser.str.ljust(20, fillchar='-'))\n",
    "print(\"------> right justify string : \\n\", ser.str.rjust(20, fillchar='-'))\n",
    "# use str.contains()\n",
    "# syntax : Series.str.contains(pat, case=True, flags=0, na=<no_default>, regex=True)\n",
    "# pat - sequence/regular expression, case - boolean default True, flags - default 0 pass through to re module re.IGNORECASE\n",
    "# na - can be update with True/False, regex - If True, assumes pat is a regular expression other false for literl string.\n",
    "s1 = pd.Series(['Mouse', 'dog', 'house and parrot', '23', np.nan])\n",
    "print(\"\\n------> check literal in series : \\n\", s1.str.contains('og', regex=False))  # user pat as string literal\n",
    "print(\"------> check regex expr in case sens : \\n\", s1.str.contains('oG', case=True, regex=True))  # when case sensitive is true\n",
    "print(\"------> check regex with set nan false : \\n\", s1.str.contains('og', na=False, regex=True)) # convert na to False value\n",
    "import re   # ignore the upper/lower case in regex\n",
    "print(\"------> check regex with igone case: \\n\", s1.str.contains('PARROT', flags=re.IGNORECASE, regex=True))\n",
    "# Use str.startswith(), str.endswith()\n",
    "# Test if the end of each string element matches a pattern.\n",
    "# syntax : Series.str.endswith(pat, na=<no_default>)\n",
    "s = pd.Series(['bat', 'bear', 'caT', np.nan])\n",
    "print(\"\\n------> check string in series end with given pattern : \\n\", s.str.endswith('t'))\n",
    "print(\"------> check string in series end with given multiple pattern : \\n\", s.str.endswith(('t', 'T')))\n",
    "print(\"------> check string in series that start with given pattern : \\n\", s.str.startswith(('b', 'B')))\n",
    "# check the count occurance in the string..\n",
    "# syntax : Series.str.count(pat, flags=0)\n",
    "s = pd.Series(['A', 'B', 'Aaba', 'Baca', np.nan, 'CABA', 'cat'])\n",
    "print(\"\\n------> count the number of occurance of pattern in strings : \\n\", s.str.count('a'))\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"_Use for more others functions/attributes related to pandas, go through below link_\n",
    "Ref : https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ------------ USE OF SELECTION (Rows/Columns), APPLY CONDITION AND FILTER (On Rows/Columns) ---------- #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"_USE FOR SELECT THE SPECIFIC COLUMNS/ROWS AND APPLIED SOME SET OF CONDITIONS AND FILTERS OR MANY MORE_\n",
    "\"\"\"\n",
    "\n",
    "# Read the titanic data...\n",
    "ttc_df = pd.read_parquet('data/titanic.parquet', engine='auto').iloc[10:30, :]\n",
    "ttc_df.reset_index(drop=True, inplace=True)\n",
    "print(\"-----> print titanic df : \\n\", ttc_df)\n",
    "\n",
    "\n",
    "# Use of Selection columns in dataframe with/without loc/iloc...\n",
    "print(\"\\n-----> select single col from df as series : \\n\", ttc_df['PassengerId'])\n",
    "print(\"\\n-----> select multiple col from df as subset df : \\n\", ttc_df[['PassengerId', 'Pclass', 'Name', 'Age']])\n",
    "print(\"\\n-----> select single col using loc with entire rows : \\n\", ttc_df.loc[:, 'Pclass'])\n",
    "print(\"\\n-----> select subset df using iloc with specific rows and cols : \\n\", ttc_df.iloc[[1, 2, 4, 6, 10, 15], [1, 2, 4]])\n",
    "\n",
    "\n",
    "# Use filter dataframe by using single/multiple/predefine_method conditions (without loc/iloc)...\n",
    "print(\"\\n------> Any Comparison/Condition on series/col, return Boolean True/False list : \\n\", (ttc_df.Age > 18))\n",
    "print(\"\\n------> filter all ttc passenger using single condition where age greater than 18 : \\n\", ttc_df[(ttc_df.Age > 18)])\n",
    "print(\"\\n-----> filter with combined condition with predefined_method : \\n\", ttc_df[(ttc_df.Age > 18) & (ttc_df['Cabin'].isnull())])\n",
    "print(\"\\n-----> filer record where age is nan : \\n\", ttc_df[ttc_df['Age'].isna()])\n",
    "\n",
    "\n",
    "# Use filter dataframe by using single/multiple/predefine_method conditions (with loc/iloc)...\n",
    "print(\"\\n-----> filter rows where 'Sex' is 'male' and select all columns \\n\", ttc_df.loc[(ttc_df.Sex=='male'), :])\n",
    "print(\"\\n-----> filter cols where match col is 'Sex' and select all rows \\n\", ttc_df.loc[:, (ttc_df.columns=='Sex')])\n",
    "print(\"\\n-----> filter row where col value present is in given specific list/dict/series/df values : \\n\",\n",
    "        ttc_df.loc[ttc_df['Parch'].isin([1, 2, 5])])\n",
    "print(\"\\n------> filter males recors where use regex pattern 'Mr.' for checking : \\n\",\n",
    "        ttc_df.loc[ttc_df['Name'].str.contains('Mr\\\\.', case=True, regex=True), ['PassengerId', 'Name', 'Age', 'Fare']])\n",
    "print(\"\\n-----> use iloc which supports interger based indexing So only filter using list of True/False values on condition :\\n\", \n",
    "        ttc_df.iloc[ttc_df['Embarked'].str.startswith(('Q', 'C')).values, :])\n",
    "\n",
    "\n",
    "# Use filter datafrane by using single/multiple/default_method conditions (with query/where/mask etc)..\n",
    "print(\"\\n-----> filter record using query like-SQL and useful for complex condition : \\n\", ttc_df.query(\"Fare > 10 and Cabin.notnull()\"))\n",
    "print(\"\\n-----> use for replaces values that do not satisfy a condition with NaN or a specified value. : \\n\",\n",
    "       ttc_df.where(ttc_df.Age > 18))\n",
    "print(\"\\n-----> use for replaces values that do satisfy a condition with NaN or a specified value. : \\n\",\n",
    "        ttc_df.mask(ttc_df.Age > 18))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"_For more reference use below link_\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.filter.html\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### --------- REFERENCES -------- #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"_MORE REFERENCES_\n",
    "\n",
    "* https://pandas.pydata.org/docs/getting_started/index.html\n",
    "* https://pandas.pydata.org/docs/user_guide/index.html\n",
    "* https://pandas.pydata.org/docs/reference/#\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
