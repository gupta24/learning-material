{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27c466bf-efbc-4fb8-9ab4-a1f12099816f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=442497685451589#setting/sparkui/0913-091544-6nz1mdhx/driver-6790830549593333023\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=442497685451589#setting/sparkui/0913-091544-6nz1mdhx/driver-6790830549593333023\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "379b5600-edbe-4051-9861-2f876682b912",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-------------- Work On Create Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "162127f8-8d8a-4927-8664-b687277c7932",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|num|\n",
      "+---+---+\n",
      "|  1|  1|\n",
      "|  2|  1|\n",
      "|  3|  1|\n",
      "|  4|  2|\n",
      "|  5|  1|\n",
      "|  6|  2|\n",
      "|  7|  2|\n",
      "+---+---+\n",
      "\n",
      "+----+---+------+-----------+\n",
      "|name|age|salary|       city|\n",
      "+----+---+------+-----------+\n",
      "|John| 25| 20000|Los Angales|\n",
      "|Mary| 32| 32000|    Chicago|\n",
      "| Bob| 21| 27000|   New York|\n",
      "| Nik| 35| 20999|      India|\n",
      "+----+---+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create dataframe normal way...\n",
    "data = [(1, 1), (2, 1), (3, 1), (4, 2), (5, 1), (6, 2), (7, 2)]\n",
    "schema = [\"id\", \"num\"]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "\n",
    "\n",
    "# create dataframe using the row keyword...\n",
    "from pyspark.sql import Row\n",
    "row_data = [Row(name='John', age=25, salary=20000, city='Los Angales'),\n",
    "        Row(name='Mary', age=32, salary=32000, city='Chicago'),\n",
    "        Row(name='Bob', age=21, salary=27000, city='New York'),\n",
    "        Row(name='Nik', age=35, salary=20999 , city='India')]\n",
    "row_schema = ['name', 'age', 'salary', 'city']\n",
    "row_df = spark.createDataFrame(row_data, row_schema)\n",
    "row_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e754b761-8be6-4ddc-9b66-6c0b7389ac01",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "------------ Work On Dataframe Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2429cae1-2f46-448c-8bff-58b86f1d9c80",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n",
      "columns is :  ['name', 'age', 'salary', 'city']\n",
      "\n",
      "\n",
      "+---+----+---+------+-----------+\n",
      "| id|name|age|salary|       city|\n",
      "+---+----+---+------+-----------+\n",
      "|  1|John| 25| 20000|Los Angales|\n",
      "|  2|Mary| 32| 32000|    Chicago|\n",
      "|  3| Bob| 21| 27000|   New York|\n",
      "|  4| Nik| 35| 20999|      India|\n",
      "+---+----+---+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the schema in the existing dataframe...\n",
    "row_df.printSchema()\n",
    "\n",
    "\n",
    "# print the column in the existing dataframe...\n",
    "print('columns is : ', row_df.columns)\n",
    "print('\\n')\n",
    "\n",
    "# create the dataframe with manual schema....\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "row_data = [\n",
    "             Row(id=1, name='John', age=25, salary=20000, city='Los Angales'),\n",
    "             Row(id=2, name='Mary', age=32, salary=32000, city='Chicago'),\n",
    "             Row(id=3, name='Bob', age=21, salary=27000, city='New York'),\n",
    "             Row(id=4, name='Nik', age=35, salary=20999 , city='India')\n",
    "            ]\n",
    "# row_schema = ['name', 'age', 'salary', 'city']  # not use this normal schema\n",
    "manual_schema = StructType([\n",
    "                    StructField('id', IntegerType(), True),\n",
    "                    StructField('name', StringType(), True),\n",
    "                    StructField('age', IntegerType(), True),\n",
    "                    StructField('salary', IntegerType(), True),\n",
    "                    StructField('city', StringType(), True)\n",
    "                ])\n",
    "\n",
    "manual_schema_df = spark.createDataFrame(row_data, manual_schema)\n",
    "manual_schema_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4435dd63-675f-4dff-9f96-2077b3c8f05f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "----------- Work On Select The Column In Different Ways.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c343a4fc-0be9-448b-a4a9-03b717e1f3b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "|John|\n",
      "|Mary|\n",
      "| Bob|\n",
      "| Nik|\n",
      "+----+\n",
      "\n",
      "+--------+\n",
      "|(id + 1)|\n",
      "+--------+\n",
      "|       2|\n",
      "|       3|\n",
      "|       4|\n",
      "|       5|\n",
      "+--------+\n",
      "\n",
      "+--------+\n",
      "|(id + 1)|\n",
      "+--------+\n",
      "|       2|\n",
      "|       3|\n",
      "|       4|\n",
      "|       5|\n",
      "+--------+\n",
      "\n",
      "+-----------+\n",
      "|       city|\n",
      "+-----------+\n",
      "|Los Angales|\n",
      "|    Chicago|\n",
      "|   New York|\n",
      "|      India|\n",
      "+-----------+\n",
      "\n",
      "+-----------+\n",
      "|       city|\n",
      "+-----------+\n",
      "|Los Angales|\n",
      "|    Chicago|\n",
      "|   New York|\n",
      "|      India|\n",
      "+-----------+\n",
      "\n",
      "+---+----+------+-----------+\n",
      "| id|name|salary|       city|\n",
      "+---+----+------+-----------+\n",
      "|  1|John| 20000|Los Angales|\n",
      "|  2|Mary| 32000|    Chicago|\n",
      "|  3| Bob| 27000|   New York|\n",
      "|  4| Nik| 20999|      India|\n",
      "+---+----+------+-----------+\n",
      "\n",
      "+---+----+---+------+-----------+\n",
      "| id|name|age|salary|       city|\n",
      "+---+----+---+------+-----------+\n",
      "|  1|John| 25| 20000|Los Angales|\n",
      "|  2|Mary| 32| 32000|    Chicago|\n",
      "|  3| Bob| 21| 27000|   New York|\n",
      "|  4| Nik| 35| 20999|      India|\n",
      "+---+----+---+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *   # col is available here..\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# select the column with only name..\n",
    "manual_schema_df.select('name').show()\n",
    "\n",
    "# select column by using col keyword.\n",
    "# col is an expression which is use to apply set of tranformation on the one or more record in specific column.\n",
    "manual_schema_df.select(col('id')+1).show()\n",
    "# same transformation apply using only column name\n",
    "# manual_schema_df.select('id' + 1).show()  \n",
    "# but it will do the issue so it will resolve using the expr keyword.\n",
    "manual_schema_df.select(expr('id') + 1).show()\n",
    "\n",
    "\n",
    "# select column using df...\n",
    "manual_schema_df.select(manual_schema_df['city']).show()\n",
    "\n",
    "# select column using df and column name..\n",
    "manual_schema_df.select(manual_schema_df.city).show()\n",
    "\n",
    "# select multiple columns\n",
    "manual_schema_df.select('id', col('name'), manual_schema_df['salary'], manual_schema_df.city).show()\n",
    "\n",
    "# select entire columns\n",
    "manual_schema_df.select('*').show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b92c23f-4ab8-4538-b008-001e899ecc89",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "--------- Work On Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d427548-5d83-49f3-b901-54ad2ca77b96",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+------+-----------+\n",
      "| id|name|age|salary|       city|\n",
      "+---+----+---+------+-----------+\n",
      "|  1|John| 25| 20000|Los Angales|\n",
      "|  2|Mary| 32| 32000|    Chicago|\n",
      "|  3| Bob| 21| 27000|   New York|\n",
      "|  4| Nik| 35| 20999|      India|\n",
      "+---+----+---+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fetch the data using spark sql, In which require to write sql query..\n",
    "# first convert the df into temporary table..\n",
    "manual_schema_df.createOrReplaceTempView(\"manual_schema_df_tbl\")\n",
    "\n",
    "# fetch the data from given table name..\n",
    "spark.sql(\"\"\"\n",
    "          select * from manual_schema_df_tbl\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e26e84ad-f9ae-4426-99c7-5e2e519e23e9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "---------- Work On Alias, Filter/Where, Literal, Adding Columns, Renaming Columns, Casting DataType, Removing Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "037ac0f2-770a-49e7-852d-659331b36904",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+------+-----------+\n",
      "| id|name|age|salary|       city|\n",
      "+---+----+---+------+-----------+\n",
      "|  1|John| 25| 20000|Los Angales|\n",
      "|  2|Mary| 32| 32000|    Chicago|\n",
      "|  3| Bob| 21| 27000|   New York|\n",
      "|  4| Nik| 35| 20999|      India|\n",
      "+---+----+---+------+-----------+\n",
      "\n",
      "+------+----+---+------+\n",
      "|emp_id|name|age|salary|\n",
      "+------+----+---+------+\n",
      "|     1|John| 25| 20000|\n",
      "|     2|Mary| 32| 32000|\n",
      "|     3| Bob| 21| 27000|\n",
      "|     4| Nik| 35| 20999|\n",
      "+------+----+---+------+\n",
      "\n",
      "+---+----+---+------+--------+\n",
      "| id|name|age|salary|    city|\n",
      "+---+----+---+------+--------+\n",
      "|  2|Mary| 32| 32000| Chicago|\n",
      "|  3| Bob| 21| 27000|New York|\n",
      "|  4| Nik| 35| 20999|   India|\n",
      "+---+----+---+------+--------+\n",
      "\n",
      "+---+----+---+------+--------+\n",
      "| id|name|age|salary|    city|\n",
      "+---+----+---+------+--------+\n",
      "|  2|Mary| 32| 32000| Chicago|\n",
      "|  3| Bob| 21| 27000|New York|\n",
      "+---+----+---+------+--------+\n",
      "\n",
      "+---+----+---+------+-----------+---------+\n",
      "| id|name|age|salary|       city|last_name|\n",
      "+---+----+---+------+-----------+---------+\n",
      "|  1|John| 25| 20000|Los Angales|     null|\n",
      "|  2|Mary| 32| 32000|    Chicago|     null|\n",
      "|  3| Bob| 21| 27000|   New York|     null|\n",
      "|  4| Nik| 35| 20999|      India|     null|\n",
      "+---+----+---+------+-----------+---------+\n",
      "\n",
      "+---+----+---+------+-----------+---------+--------+\n",
      "| id|name|age|salary|       city|last_name|sur_name|\n",
      "+---+----+---+------+-----------+---------+--------+\n",
      "|  1|John| 25| 20000|Los Angales|      deo|    null|\n",
      "|  2|Mary| 32| 32000|    Chicago|      deo|    null|\n",
      "|  3| Bob| 21| 27000|   New York|      deo|    null|\n",
      "|  4| Nik| 35| 20999|      India|      deo|    null|\n",
      "+---+----+---+------+-----------+---------+--------+\n",
      "\n",
      "+------+----+---+------+-----------+---------+--------+\n",
      "|emp_id|name|age|salary|       city|last_name|sur_name|\n",
      "+------+----+---+------+-----------+---------+--------+\n",
      "|     1|John| 25| 20000|Los Angales|      deo|    null|\n",
      "|     2|Mary| 32| 32000|    Chicago|      deo|    null|\n",
      "|     3| Bob| 21| 27000|   New York|      deo|    null|\n",
      "|     4| Nik| 35| 20999|      India|      deo|    null|\n",
      "+------+----+---+------+-----------+---------+--------+\n",
      "\n",
      "root\n",
      " |-- emp_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- last_name: string (nullable = false)\n",
      " |-- sur_name: string (nullable = false)\n",
      "\n",
      "+------+----+---+------+-----------+---------+\n",
      "|emp_id|name|age|salary|       city|last_name|\n",
      "+------+----+---+------+-----------+---------+\n",
      "|     1|John| 25| 20000|Los Angales|      deo|\n",
      "|     2|Mary| 32| 32000|    Chicago|      deo|\n",
      "|     3| Bob| 21| 27000|   New York|      deo|\n",
      "|     4| Nik| 35| 20999|      India|      deo|\n",
      "+------+----+---+------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# alias the dataframe and column name..\n",
    "# alias is used temporary changes. and rename is used for parmanent changes of column name.\n",
    "ms_df = manual_schema_df.alias('ms_df')\n",
    "ms_df.show()\n",
    "ms_df.select(col('id').alias('emp_id'), 'name', 'age', 'salary').show()\n",
    "\n",
    "\n",
    "\n",
    "# use filter / where to find out the satisfied condition records..\n",
    "ms_df.filter(col('salary') > 20000).show()\n",
    "ms_df.filter((col('salary') > 20000) & (col('age') < 35)).show()\n",
    "\n",
    "\n",
    "\n",
    "# use literal for create the new column in existing dataframe with default/null value..\n",
    "ms_df = ms_df.select('*', lit('null').alias('last_name'))\n",
    "ms_df.show()\n",
    "\n",
    "\n",
    "# adding or overwrite the columns in existing dataframe..\n",
    "ms_df = ms_df.withColumn('last_name', lit('deo'))  # overwrite\n",
    "ms_df = ms_df.withColumn('sur_name', lit('null'))# create new\n",
    "ms_df.show()\n",
    "\n",
    "\n",
    "# rename column in existing dataframe..\n",
    "ms_df = ms_df.withColumnRenamed('id', 'emp_id')\n",
    "ms_df.show()\n",
    "\n",
    "\n",
    "# casting the column datatype in existing dataframe..\n",
    "ms_df = ms_df.withColumn('emp_id', col('emp_id').cast('string'))\n",
    "ms_df.withColumn('salary', col('salary').cast('long')).printSchema()\n",
    "\n",
    "\n",
    "# remove the existing column in dataframe..\n",
    "ms_df.drop('sur_name').show()\n",
    "\n",
    "\n",
    "# Usage of lit() with withColumn()\n",
    "from pyspark.sql.functions import when, lit, col\n",
    "df3 = ms_df.withColumn(\"lit_value2\", when((col(\"salary\") >=40000) & (col(\"salary\") <= 50000),lit(\"100\")).otherwise(lit(\"200\")))\n",
    "df3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa938e9b-ab87-4ba0-82d9-f0b1899149b1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "------------ Use Above Same Thing In Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "517ce786-c005-432f-8256-a1cb1fa81487",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+------+-----------+\n",
      "| id|name|age|salary|       city|\n",
      "+---+----+---+------+-----------+\n",
      "|  1|John| 25| 20000|Los Angales|\n",
      "|  2|Mary| 32| 32000|    Chicago|\n",
      "|  3| Bob| 21| 27000|   New York|\n",
      "|  4| Nik| 35| 20999|      India|\n",
      "+---+----+---+------+-----------+\n",
      "\n",
      "+------+----+---+------+---------+\n",
      "|emp_id|name|age|salary|last_name|\n",
      "+------+----+---+------+---------+\n",
      "|     2|Mary| 32| 32000|      deo|\n",
      "|     3| Bob| 21| 27000|      deo|\n",
      "|     4| Nik| 35| 20999|      deo|\n",
      "+------+----+---+------+---------+\n",
      "\n",
      "root\n",
      " |-- emp_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- last_name: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fetch the data from given table name..\n",
    "spark.sql(\"\"\"\n",
    "          select * from manual_schema_df_tbl\n",
    "          \"\"\").show()\n",
    "\n",
    "\n",
    "# use alias, filter, casting, adding new column etc.\n",
    "spark.sql(\"\"\"\n",
    "          select id as emp_id, name, cast(age as long), salary, 'deo' as last_name from manual_schema_df_tbl\n",
    "          where salary > 20000\n",
    "          \"\"\").show()\n",
    "\n",
    "# show the schema..\n",
    "spark.sql(\"\"\"\n",
    "          select id as emp_id, name, cast(age as long), 'deo' as last_name from manual_schema_df_tbl\n",
    "          \"\"\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bf75ccd-496f-4425-b157-d758daa14ec6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "create_dataframe_operations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
