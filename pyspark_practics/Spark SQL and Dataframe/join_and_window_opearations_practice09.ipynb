{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "799efb49-c643-4c19-a68e-f2ff2027a260",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=442497685451589#setting/sparkui/0924-060329-cqwt1r9q/driver-5567274119822897344\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=442497685451589#setting/sparkui/0924-060329-cqwt1r9q/driver-5567274119822897344\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49d763a9-974a-4646-bb72-67061d54264d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "------------- Create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83a37c66-a72d-4c0a-8080-fb151a847c60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+\n|customer_id|customer_name|address|date_of_joining|\n+-----------+-------------+-------+---------------+\n|          1|       manish|  patna|     30-05-2022|\n|          2|       vikash|kolkata|     12-03-2023|\n|          3|       nikita|  delhi|     25-06-2023|\n|          4|        rahul| ranchi|     24-03-2023|\n|          5|       mahesh| jaipur|     22-03-2023|\n|          6|     prantosh|kolkata|     18-10-2022|\n|          7|        raman|  patna|     30-12-2022|\n|          8|      prakash| ranchi|     24-02-2023|\n|          9|       ragini|kolkata|     03-03-2023|\n|         10|      raushan| jaipur|     05-02-2023|\n+-----------+-------------+-------+---------------+\n\n+-----------+----------+--------+----------------+\n|customer_id|product_id|quantity|date_of_purchase|\n+-----------+----------+--------+----------------+\n|          1|        22|      10|      01-06-2022|\n|          1|        27|       5|      03-02-2023|\n|          2|         5|       3|      01-06-2023|\n|          5|        22|       1|      22-03-2023|\n|          7|        22|       4|      03-02-2023|\n|          9|         5|       6|      03-03-2023|\n|          2|         1|      12|      15-06-2023|\n|          1|        56|       2|      25-06-2023|\n|          5|        12|       5|      15-04-2023|\n|         11|        12|      76|      12-03-2023|\n+-----------+----------+--------+----------------+\n\n+---+-------+-----+\n| id|   name|price|\n+---+-------+-----+\n|  1|  fanta|   20|\n|  2|    dew|   22|\n|  5| sprite|   40|\n|  7|redbull|  100|\n| 12|  mazza|   45|\n| 22|   coke|   27|\n| 25|  limca|   21|\n| 27|  pepsi|   14|\n| 56|  sting|   10|\n+---+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# create the customer data...\n",
    "customer_data = [(1,'manish','patna',\"30-05-2022\"),\n",
    "                (2,'vikash','kolkata',\"12-03-2023\"),\n",
    "                (3,'nikita','delhi',\"25-06-2023\"),\n",
    "                (4,'rahul','ranchi',\"24-03-2023\"),\n",
    "                (5,'mahesh','jaipur',\"22-03-2023\"),\n",
    "                (6,'prantosh','kolkata',\"18-10-2022\"),\n",
    "                (7,'raman','patna',\"30-12-2022\"),\n",
    "                (8,'prakash','ranchi',\"24-02-2023\"),\n",
    "                (9,'ragini','kolkata',\"03-03-2023\"),\n",
    "                (10,'raushan','jaipur',\"05-02-2023\")]\n",
    "customer_schema=['customer_id','customer_name','address','date_of_joining']\n",
    "cust_df = spark.createDataFrame(customer_data, customer_schema)\n",
    "cust_df.show()\n",
    "\n",
    "\n",
    "# creata the sales data..\n",
    "sales_data = [(1,22,10,\"01-06-2022\"),\n",
    "            (1,27,5,\"03-02-2023\"),\n",
    "            (2,5,3,\"01-06-2023\"),\n",
    "            (5,22,1,\"22-03-2023\"),\n",
    "            (7,22,4,\"03-02-2023\"),\n",
    "            (9,5,6,\"03-03-2023\"),\n",
    "            (2,1,12,\"15-06-2023\"),\n",
    "            (1,56,2,\"25-06-2023\"),\n",
    "            (5,12,5,\"15-04-2023\"),\n",
    "            (11,12,76,\"12-03-2023\")]\n",
    "sales_schema=['customer_id','product_id','quantity','date_of_purchase']\n",
    "sales_df = spark.createDataFrame(sales_data, sales_schema)\n",
    "sales_df.show()\n",
    "\n",
    "\n",
    "# create the product data... \n",
    "product_data = [(1, 'fanta',20),\n",
    "                (2, 'dew',22),\n",
    "                (5, 'sprite',40),\n",
    "                (7, 'redbull',100),\n",
    "                (12,'mazza',45),\n",
    "                (22,'coke',27),\n",
    "                (25,'limca',21),\n",
    "                (27,'pepsi',14),\n",
    "                (56,'sting',10)]\n",
    "product_schema=['id','name','price']\n",
    "prod_df = spark.createDataFrame(product_data, product_schema)\n",
    "prod_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2878903f-1e8a-48e8-a151-bade91039208",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-------------- Use Simple Inner Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a01ecf08-0545-4f9b-803d-48c0521e7a11",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n|customer_id|customer_name|address|date_of_joining|customer_id|product_id|quantity|date_of_purchase|\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n|          1|       manish|  patna|     30-05-2022|          1|        22|      10|      01-06-2022|\n|          1|       manish|  patna|     30-05-2022|          1|        27|       5|      03-02-2023|\n|          1|       manish|  patna|     30-05-2022|          1|        56|       2|      25-06-2023|\n|          2|       vikash|kolkata|     12-03-2023|          2|         5|       3|      01-06-2023|\n|          2|       vikash|kolkata|     12-03-2023|          2|         1|      12|      15-06-2023|\n|          5|       mahesh| jaipur|     22-03-2023|          5|        22|       1|      22-03-2023|\n|          5|       mahesh| jaipur|     22-03-2023|          5|        12|       5|      15-04-2023|\n|          7|        raman|  patna|     30-12-2022|          7|        22|       4|      03-02-2023|\n|          9|       ragini|kolkata|     03-03-2023|          9|         5|       6|      03-03-2023|\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# apply join with customer and sale data frame..\n",
    "cust_df.alias('ct').join(sales_df.alias('st'), col('ct.customer_id')==col('st.customer_id'), 'inner').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2362a5a-bc32-406f-a40f-806932bcc83f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-------------- Join Two or More Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac0dd017-29c2-4904-af51-8a640ddca9a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+----------+--------+----------------+------+-----+\n|customer_id|customer_name|address|date_of_joining|product_id|quantity|date_of_purchase|  name|price|\n+-----------+-------------+-------+---------------+----------+--------+----------------+------+-----+\n|          1|       manish|  patna|     30-05-2022|        22|      10|      01-06-2022|  coke|   27|\n|          1|       manish|  patna|     30-05-2022|        27|       5|      03-02-2023| pepsi|   14|\n|          1|       manish|  patna|     30-05-2022|        56|       2|      25-06-2023| sting|   10|\n|          2|       vikash|kolkata|     12-03-2023|         5|       3|      01-06-2023|sprite|   40|\n|          2|       vikash|kolkata|     12-03-2023|         1|      12|      15-06-2023| fanta|   20|\n|          5|       mahesh| jaipur|     22-03-2023|        12|       5|      15-04-2023| mazza|   45|\n|          5|       mahesh| jaipur|     22-03-2023|        22|       1|      22-03-2023|  coke|   27|\n|          7|        raman|  patna|     30-12-2022|        22|       4|      03-02-2023|  coke|   27|\n|          9|       ragini|kolkata|     03-03-2023|         5|       6|      03-03-2023|sprite|   40|\n+-----------+-------------+-------+---------------+----------+--------+----------------+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# apply join with customer and sale data frame..\n",
    "cust_df.alias('ct').join(\n",
    "                        sales_df.alias('st'),\n",
    "                        col('ct.customer_id')==col('st.customer_id'),\n",
    "                        'inner'\n",
    "                    ).join(\n",
    "                        prod_df.alias('pt'),\n",
    "                        col('st.product_id') == col('pt.id'),\n",
    "                        'inner'\n",
    "                    ).select(\n",
    "                        col('ct.customer_id'),\n",
    "                        col('ct.customer_name'),    \n",
    "                        col('ct.address'),\n",
    "                        col('ct.date_of_joining'),\n",
    "                        col('st.product_id'),\n",
    "                        col('st.quantity'),\n",
    "                        col('st.date_of_purchase'),\n",
    "                        col('pt.name'),\n",
    "                        col('pt.price'),\n",
    "                    ).sort(\n",
    "                        col('ct.customer_id')\n",
    "                    ).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33c553c7-433d-411c-b5dc-dd965105b53f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "--------------- Work With Others Types (Left, Right, Full Outer, Left Semi, Left Anti, Cross) Of Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f55541a-f36a-4c8e-a84e-07580c32ac21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n|customer_id|customer_name|address|date_of_joining|customer_id|product_id|quantity|date_of_purchase|\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n|          1|       manish|  patna|     30-05-2022|          1|        56|       2|      25-06-2023|\n|          1|       manish|  patna|     30-05-2022|          1|        27|       5|      03-02-2023|\n|          1|       manish|  patna|     30-05-2022|          1|        22|      10|      01-06-2022|\n|          2|       vikash|kolkata|     12-03-2023|          2|         1|      12|      15-06-2023|\n|          2|       vikash|kolkata|     12-03-2023|          2|         5|       3|      01-06-2023|\n|          3|       nikita|  delhi|     25-06-2023|       null|      null|    null|            null|\n|          5|       mahesh| jaipur|     22-03-2023|          5|        12|       5|      15-04-2023|\n|          5|       mahesh| jaipur|     22-03-2023|          5|        22|       1|      22-03-2023|\n|          4|        rahul| ranchi|     24-03-2023|       null|      null|    null|            null|\n|          6|     prantosh|kolkata|     18-10-2022|       null|      null|    null|            null|\n|          7|        raman|  patna|     30-12-2022|          7|        22|       4|      03-02-2023|\n|          8|      prakash| ranchi|     24-02-2023|       null|      null|    null|            null|\n|          9|       ragini|kolkata|     03-03-2023|          9|         5|       6|      03-03-2023|\n|         10|      raushan| jaipur|     05-02-2023|       null|      null|    null|            null|\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n\n+-----------+----------+--------+----------------+---+-------+-----+\n|customer_id|product_id|quantity|date_of_purchase| id|   name|price|\n+-----------+----------+--------+----------------+---+-------+-----+\n|          2|         1|      12|      15-06-2023|  1|  fanta|   20|\n|       null|      null|    null|            null|  2|    dew|   22|\n|          9|         5|       6|      03-03-2023|  5| sprite|   40|\n|          2|         5|       3|      01-06-2023|  5| sprite|   40|\n|       null|      null|    null|            null|  7|redbull|  100|\n|         11|        12|      76|      12-03-2023| 12|  mazza|   45|\n|          5|        12|       5|      15-04-2023| 12|  mazza|   45|\n|          7|        22|       4|      03-02-2023| 22|   coke|   27|\n|          5|        22|       1|      22-03-2023| 22|   coke|   27|\n|          1|        22|      10|      01-06-2022| 22|   coke|   27|\n|       null|      null|    null|            null| 25|  limca|   21|\n|          1|        27|       5|      03-02-2023| 27|  pepsi|   14|\n|          1|        56|       2|      25-06-2023| 56|  sting|   10|\n+-----------+----------+--------+----------------+---+-------+-----+\n\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n|customer_id|customer_name|address|date_of_joining|customer_id|product_id|quantity|date_of_purchase|\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n|          1|       manish|  patna|     30-05-2022|          1|        22|      10|      01-06-2022|\n|          1|       manish|  patna|     30-05-2022|          1|        27|       5|      03-02-2023|\n|          1|       manish|  patna|     30-05-2022|          1|        56|       2|      25-06-2023|\n|          2|       vikash|kolkata|     12-03-2023|          2|         5|       3|      01-06-2023|\n|          2|       vikash|kolkata|     12-03-2023|          2|         1|      12|      15-06-2023|\n|          3|       nikita|  delhi|     25-06-2023|       null|      null|    null|            null|\n|          4|        rahul| ranchi|     24-03-2023|       null|      null|    null|            null|\n|          5|       mahesh| jaipur|     22-03-2023|          5|        22|       1|      22-03-2023|\n|          5|       mahesh| jaipur|     22-03-2023|          5|        12|       5|      15-04-2023|\n|          6|     prantosh|kolkata|     18-10-2022|       null|      null|    null|            null|\n|          7|        raman|  patna|     30-12-2022|          7|        22|       4|      03-02-2023|\n|          8|      prakash| ranchi|     24-02-2023|       null|      null|    null|            null|\n|          9|       ragini|kolkata|     03-03-2023|          9|         5|       6|      03-03-2023|\n|         10|      raushan| jaipur|     05-02-2023|       null|      null|    null|            null|\n|       null|         null|   null|           null|         11|        12|      76|      12-03-2023|\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n\n+-----------+-------------+-------+---------------+\n|customer_id|customer_name|address|date_of_joining|\n+-----------+-------------+-------+---------------+\n|          1|       manish|  patna|     30-05-2022|\n|          2|       vikash|kolkata|     12-03-2023|\n|          5|       mahesh| jaipur|     22-03-2023|\n|          7|        raman|  patna|     30-12-2022|\n|          9|       ragini|kolkata|     03-03-2023|\n+-----------+-------------+-------+---------------+\n\n+-----------+-------------+-------+---------------+\n|customer_id|customer_name|address|date_of_joining|\n+-----------+-------------+-------+---------------+\n|          3|       nikita|  delhi|     25-06-2023|\n|          4|        rahul| ranchi|     24-03-2023|\n|          6|     prantosh|kolkata|     18-10-2022|\n|          8|      prakash| ranchi|     24-02-2023|\n|         10|      raushan| jaipur|     05-02-2023|\n+-----------+-------------+-------+---------------+\n\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n|customer_id|customer_name|address|date_of_joining|customer_id|product_id|quantity|date_of_purchase|\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\n|          1|       manish|  patna|     30-05-2022|          1|        22|      10|      01-06-2022|\n|          1|       manish|  patna|     30-05-2022|          1|        27|       5|      03-02-2023|\n|          1|       manish|  patna|     30-05-2022|          2|         5|       3|      01-06-2023|\n|          1|       manish|  patna|     30-05-2022|          5|        22|       1|      22-03-2023|\n|          1|       manish|  patna|     30-05-2022|          7|        22|       4|      03-02-2023|\n|          1|       manish|  patna|     30-05-2022|          9|         5|       6|      03-03-2023|\n|          1|       manish|  patna|     30-05-2022|          2|         1|      12|      15-06-2023|\n|          1|       manish|  patna|     30-05-2022|          1|        56|       2|      25-06-2023|\n|          1|       manish|  patna|     30-05-2022|          5|        12|       5|      15-04-2023|\n|          1|       manish|  patna|     30-05-2022|         11|        12|      76|      12-03-2023|\n|          2|       vikash|kolkata|     12-03-2023|          1|        22|      10|      01-06-2022|\n|          2|       vikash|kolkata|     12-03-2023|          1|        27|       5|      03-02-2023|\n|          2|       vikash|kolkata|     12-03-2023|          2|         5|       3|      01-06-2023|\n|          2|       vikash|kolkata|     12-03-2023|          5|        22|       1|      22-03-2023|\n|          2|       vikash|kolkata|     12-03-2023|          7|        22|       4|      03-02-2023|\n|          2|       vikash|kolkata|     12-03-2023|          9|         5|       6|      03-03-2023|\n|          2|       vikash|kolkata|     12-03-2023|          2|         1|      12|      15-06-2023|\n|          2|       vikash|kolkata|     12-03-2023|          1|        56|       2|      25-06-2023|\n|          2|       vikash|kolkata|     12-03-2023|          5|        12|       5|      15-04-2023|\n|          2|       vikash|kolkata|     12-03-2023|         11|        12|      76|      12-03-2023|\n+-----------+-------------+-------+---------------+-----------+----------+--------+----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# use left join...\n",
    "cust_df.join(\n",
    "            sales_df,\n",
    "            cust_df.customer_id == sales_df.customer_id,\n",
    "            'left'\n",
    "        ).show()\n",
    "\n",
    "\n",
    "# use right join...\n",
    "sales_df.join(\n",
    "            prod_df,\n",
    "            sales_df.product_id == prod_df.id,\n",
    "            'right'\n",
    "        ).show()\n",
    "\n",
    "\n",
    "# use outer join...\n",
    "cust_df.join(\n",
    "            sales_df,\n",
    "            cust_df.customer_id == sales_df.customer_id,\n",
    "            'outer'\n",
    "        ).show()\n",
    "\n",
    "\n",
    "# use left semi join...\n",
    "cust_df.join(\n",
    "            sales_df,\n",
    "            cust_df.customer_id == sales_df.customer_id,\n",
    "            'left_semi'\n",
    "        ).show()\n",
    "    \n",
    "\n",
    "# use left anti join...\n",
    "cust_df.join(\n",
    "            sales_df,\n",
    "            cust_df.customer_id == sales_df.customer_id,\n",
    "            'left_anti'\n",
    "        ).show()\n",
    "\n",
    "\n",
    "# use cross join or cartision product...\n",
    "cust_df.crossJoin(\n",
    "            sales_df\n",
    "        ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0fb4556-8c7d-4397-9221-e00a9a29c7f9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "---------------- Use Window Functions (Row_Number, Rank, Dense_Rank, Lead, Lag, First, last, Nth, rowBetween and rangeBetween)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b374277f-d603-4da0-af68-608b2e9805af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---------------+----------+--------+----------------+\n|customer_id|customer_name|address|date_of_joining|product_id|quantity|date_of_purchase|\n+-----------+-------------+-------+---------------+----------+--------+----------------+\n|          1|       manish|  patna|     30-05-2022|        56|       2|      25-06-2023|\n|          1|       manish|  patna|     30-05-2022|        27|       5|      03-02-2023|\n|          1|       manish|  patna|     30-05-2022|        22|      10|      01-06-2022|\n|          2|       vikash|kolkata|     12-03-2023|         1|      12|      15-06-2023|\n|          2|       vikash|kolkata|     12-03-2023|         5|       3|      01-06-2023|\n|          3|       nikita|  delhi|     25-06-2023|      null|    null|            null|\n|          4|        rahul| ranchi|     24-03-2023|      null|    null|            null|\n|          5|       mahesh| jaipur|     22-03-2023|        12|       5|      15-04-2023|\n|          5|       mahesh| jaipur|     22-03-2023|        22|       1|      22-03-2023|\n|          6|     prantosh|kolkata|     18-10-2022|      null|    null|            null|\n|          7|        raman|  patna|     30-12-2022|        22|       4|      03-02-2023|\n|          8|      prakash| ranchi|     24-02-2023|      null|    null|            null|\n|          9|       ragini|kolkata|     03-03-2023|         5|       6|      03-03-2023|\n|         10|      raushan| jaipur|     05-02-2023|      null|    null|            null|\n+-----------+-------------+-------+---------------+----------+--------+----------------+\n\n+-----------+-------------+--------------+\n|customer_id|customer_name|total_quantity|\n+-----------+-------------+--------------+\n|          1|       manish|            17|\n|          2|       vikash|            15|\n|          3|       nikita|          null|\n|          5|       mahesh|             6|\n|          4|        rahul|          null|\n|          6|     prantosh|          null|\n|          7|        raman|             4|\n|          8|      prakash|          null|\n|          9|       ragini|             6|\n|         10|      raushan|          null|\n+-----------+-------------+--------------+\n\n+-----------+-------------+-------+---------------+----------+--------+----------------+----------+----+----------+\n|customer_id|customer_name|address|date_of_joining|product_id|quantity|date_of_purchase|row_number|rank|dense_rank|\n+-----------+-------------+-------+---------------+----------+--------+----------------+----------+----+----------+\n|          1|       manish|  patna|     30-05-2022|        56|       2|      25-06-2023|         1|   1|         1|\n|          1|       manish|  patna|     30-05-2022|        27|       5|      03-02-2023|         2|   1|         1|\n|          1|       manish|  patna|     30-05-2022|        22|      10|      01-06-2022|         3|   1|         1|\n|          2|       vikash|kolkata|     12-03-2023|         1|      12|      15-06-2023|         1|   4|         2|\n|          2|       vikash|kolkata|     12-03-2023|         5|       3|      01-06-2023|         2|   4|         2|\n|          3|       nikita|  delhi|     25-06-2023|      null|    null|            null|         1|   6|         3|\n|          4|        rahul| ranchi|     24-03-2023|      null|    null|            null|         1|   7|         4|\n|          5|       mahesh| jaipur|     22-03-2023|        12|       5|      15-04-2023|         1|   8|         5|\n|          5|       mahesh| jaipur|     22-03-2023|        22|       1|      22-03-2023|         2|   8|         5|\n|          6|     prantosh|kolkata|     18-10-2022|      null|    null|            null|         1|  10|         6|\n|          7|        raman|  patna|     30-12-2022|        22|       4|      03-02-2023|         1|  11|         7|\n|          8|      prakash| ranchi|     24-02-2023|      null|    null|            null|         1|  12|         8|\n|          9|       ragini|kolkata|     03-03-2023|         5|       6|      03-03-2023|         1|  13|         9|\n|         10|      raushan| jaipur|     05-02-2023|      null|    null|            null|         1|  14|        10|\n+-----------+-------------+-------+---------------+----------+--------+----------------+----------+----+----------+\n\n+-----------+----------+--------+----------------+---------+----------+---------+--------------+--------------------------+\n|customer_id|product_id|quantity|date_of_purchase|prod_name|unit_price|total_amt|previous_sales|sales_gain_or_loss_percent|\n+-----------+----------+--------+----------------+---------+----------+---------+--------------+--------------------------+\n|          2|         1|      12|      15-06-2023|    fanta|        20|      240|          null|                      null|\n|          2|         5|       3|      01-06-2023|   sprite|        40|      120|          null|                      null|\n|          9|         5|       6|      03-03-2023|   sprite|        40|      240|           120|                      50.0|\n|         11|        12|      76|      12-03-2023|    mazza|        45|     3420|          null|                      null|\n|          5|        12|       5|      15-04-2023|    mazza|        45|      225|          3420|                   -1420.0|\n|          1|        22|      10|      01-06-2022|     coke|        27|      270|          null|                      null|\n|          7|        22|       4|      03-02-2023|     coke|        27|      108|           270|                    -150.0|\n|          5|        22|       1|      22-03-2023|     coke|        27|       27|           108|                    -300.0|\n|          1|        27|       5|      03-02-2023|    pepsi|        14|       70|          null|                      null|\n|          1|        56|       2|      25-06-2023|    sting|        10|       20|          null|                      null|\n+-----------+----------+--------+----------------+---------+----------+---------+--------------+--------------------------+\n\n+-----------+----------+--------+----------------+---------+----------+---------+----------------------+---------------------+\n|customer_id|product_id|quantity|date_of_purchase|prod_name|unit_price|total_amt|first_amt_of_each_prod|last_amt_of_each_prod|\n+-----------+----------+--------+----------------+---------+----------+---------+----------------------+---------------------+\n|          2|         1|      12|      15-06-2023|    fanta|        20|      240|                   240|                  240|\n|          2|         5|       3|      01-06-2023|   sprite|        40|      120|                   120|                  120|\n|          9|         5|       6|      03-03-2023|   sprite|        40|      240|                   120|                  240|\n|         11|        12|      76|      12-03-2023|    mazza|        45|     3420|                  3420|                 3420|\n|          5|        12|       5|      15-04-2023|    mazza|        45|      225|                  3420|                  225|\n|          1|        22|      10|      01-06-2022|     coke|        27|      270|                   270|                  270|\n|          7|        22|       4|      03-02-2023|     coke|        27|      108|                   270|                  108|\n|          5|        22|       1|      22-03-2023|     coke|        27|       27|                   270|                   27|\n|          1|        27|       5|      03-02-2023|    pepsi|        14|       70|                    70|                   70|\n|          1|        56|       2|      25-06-2023|    sting|        10|       20|                    20|                   20|\n+-----------+----------+--------+----------------+---------+----------+---------+----------------------+---------------------+\n\n+-----------+----------+--------+----------------+---------+----------+---------+----------------------+---------------------+\n|customer_id|product_id|quantity|date_of_purchase|prod_name|unit_price|total_amt|first_amt_of_each_prod|last_amt_of_each_prod|\n+-----------+----------+--------+----------------+---------+----------+---------+----------------------+---------------------+\n|          2|         1|      12|      15-06-2023|    fanta|        20|      240|                   240|                  240|\n|          2|         5|       3|      01-06-2023|   sprite|        40|      120|                   120|                  240|\n|          9|         5|       6|      03-03-2023|   sprite|        40|      240|                   120|                  240|\n|         11|        12|      76|      12-03-2023|    mazza|        45|     3420|                  3420|                  225|\n|          5|        12|       5|      15-04-2023|    mazza|        45|      225|                  3420|                  225|\n|          1|        22|      10|      01-06-2022|     coke|        27|      270|                   270|                   27|\n|          7|        22|       4|      03-02-2023|     coke|        27|      108|                   270|                   27|\n|          5|        22|       1|      22-03-2023|     coke|        27|       27|                   270|                   27|\n|          1|        27|       5|      03-02-2023|    pepsi|        14|       70|                    70|                   70|\n|          1|        56|       2|      25-06-2023|    sting|        10|       20|                    20|                   20|\n+-----------+----------+--------+----------------+---------+----------+---------+----------------------+---------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# use group by after join in df..\n",
    "join_df = cust_df.join(\n",
    "                sales_df,\n",
    "                cust_df.customer_id == sales_df.customer_id,\n",
    "                'left'\n",
    "            ).select(\n",
    "                cust_df.customer_id,\n",
    "                cust_df.customer_name,\n",
    "                cust_df.address,\n",
    "                cust_df.date_of_joining,\n",
    "                sales_df.product_id,\n",
    "                sales_df.quantity,\n",
    "                sales_df.date_of_purchase\n",
    "            )\n",
    "join_df.show()\n",
    "join_df.groupBy(\n",
    "            join_df.customer_id, join_df.customer_name\n",
    "        ).agg(\n",
    "            sum(join_df.quantity).alias('total_quantity')\n",
    "        ).show()\n",
    "\n",
    "\n",
    "\n",
    "# import window library\n",
    "from pyspark.sql.window import Window\n",
    "# use row_number to set unique number for each group as window..\n",
    "join_df.withColumn('row_number', row_number().over(Window.partitionBy('customer_id').orderBy('customer_id')))\\\n",
    "       .withColumn('rank', rank().over(Window.orderBy('customer_id')))\\\n",
    "       .withColumn('dense_rank', dense_rank().over(Window.orderBy('customer_id')))\\\n",
    "       .sort('customer_id')\\\n",
    "       .show()\n",
    "\n",
    "\n",
    "# use lead and lag, first and last on the sales data..\n",
    "from pyspark.sql.functions import *\n",
    "final_sales_df = sales_df.join(\n",
    "                            prod_df,\n",
    "                            sales_df.product_id == prod_df.id,\n",
    "                            'inner'\n",
    "                        ).select(\n",
    "                            sales_df.customer_id,\n",
    "                            sales_df.product_id,\n",
    "                            sales_df.quantity,\n",
    "                            sales_df.date_of_purchase,\n",
    "                            prod_df.name.alias('prod_name'),\n",
    "                            prod_df.price.alias('unit_price'),\n",
    "                            (sales_df.quantity * prod_df.price).alias('total_amt')\n",
    "                        )\n",
    "\n",
    "\n",
    "# use lag (work with backword value) and similarly lead (work wih forward value) as window function..                        \n",
    "final_sales_df.withColumn(\n",
    "                'previous_sales',\n",
    "                lag('total_amt', 1).over(\n",
    "                    Window.partitionBy('product_id').orderBy('date_of_purchase')\n",
    "                )\n",
    "            ).withColumn(\n",
    "                'sales_gain_or_loss_percent',\n",
    "                (((col('total_amt') - col('previous_sales'))/col('total_amt'))*100)       \n",
    "            ).show()\n",
    "\n",
    "\n",
    "# use first (return the first value from each window) and last (return the last value from each window) as window function.\n",
    "final_sales_df.withColumn(\n",
    "                'first_amt_of_each_prod',\n",
    "                first('total_amt').over(\n",
    "                    Window.partitionBy('product_id').orderBy('date_of_purchase')\n",
    "                )\n",
    "            ).withColumn(\n",
    "                'last_amt_of_each_prod',\n",
    "                last('total_amt').over(\n",
    "                    Window.partitionBy('product_id').orderBy('date_of_purchase')\n",
    "                )     \n",
    "            ).show()\n",
    "\n",
    "\n",
    "# use rowBetween and rangeBetween with first and last as window function\n",
    "\"\"\"\n",
    "because if we check that find first and last value in previous step.. then last values is getting as current value\n",
    "of each window where first work as unboundedPreceding value and last work as current value  in each window.\n",
    "So we will use rowBetween with first and last then first work as unboundedPreceding value and last work as unboundedFollowing\n",
    "value in each window.\n",
    "\"\"\"\n",
    "final_sales_df.withColumn(\n",
    "                'first_amt_of_each_prod',\n",
    "                first('total_amt').over(\n",
    "                    Window.partitionBy('product_id').orderBy('date_of_purchase').rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "                )\n",
    "            ).withColumn(\n",
    "                'last_amt_of_each_prod',\n",
    "                last('total_amt').over(\n",
    "                    Window.partitionBy('product_id').orderBy('date_of_purchase').rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "                )     \n",
    "            ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87f61b23-7dac-4821-96e8-d9a579e527a1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-------------- Work On Different Data To Know More About The RowBetween And RangeBetween"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19750e70-1653-4f03-b77a-af1604c00c00",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+-----+\n| id|  name|      date| time|\n+---+------+----------+-----+\n|  1|manish|11-07-2023|10:20|\n|  1|manish|11-07-2023|11:20|\n|  2|rajesh|11-07-2023|11:20|\n|  1|manish|11-07-2023|11:50|\n|  2|rajesh|11-07-2023|13:20|\n|  1|manish|11-07-2023|19:20|\n|  2|rajesh|11-07-2023|17:20|\n|  1|manish|12-07-2023|10:32|\n|  1|manish|12-07-2023|12:20|\n|  3|vikash|12-07-2023|09:12|\n|  1|manish|12-07-2023|16:23|\n|  3|vikash|12-07-2023|18:08|\n+---+------+----------+-----+\n\n+---+------+----------+-----+-------------------+\n| id|  name|      date| time|          timestamp|\n+---+------+----------+-----+-------------------+\n|  1|manish|11-07-2023|10:20|2023-07-11 10:20:00|\n|  1|manish|11-07-2023|11:20|2023-07-11 11:20:00|\n|  2|rajesh|11-07-2023|11:20|2023-07-11 11:20:00|\n|  1|manish|11-07-2023|11:50|2023-07-11 11:50:00|\n|  2|rajesh|11-07-2023|13:20|2023-07-11 13:20:00|\n|  1|manish|11-07-2023|19:20|2023-07-11 19:20:00|\n|  2|rajesh|11-07-2023|17:20|2023-07-11 17:20:00|\n|  1|manish|12-07-2023|10:32|2023-07-12 10:32:00|\n|  1|manish|12-07-2023|12:20|2023-07-12 12:20:00|\n|  3|vikash|12-07-2023|09:12|2023-07-12 09:12:00|\n|  1|manish|12-07-2023|16:23|2023-07-12 16:23:00|\n|  3|vikash|12-07-2023|18:08|2023-07-12 18:08:00|\n+---+------+----------+-----+-------------------+\n\nroot\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- date: string (nullable = true)\n |-- time: string (nullable = true)\n |-- timestamp: timestamp (nullable = true)\n\n+---+------+----------+-----+-------------------+-------------------+-------------------+\n| id|  name|      date| time|          timestamp|         entry time|          exit time|\n+---+------+----------+-----+-------------------+-------------------+-------------------+\n|  1|manish|11-07-2023|10:20|2023-07-11 10:20:00|2023-07-11 10:20:00|2023-07-11 19:20:00|\n|  1|manish|11-07-2023|11:20|2023-07-11 11:20:00|2023-07-11 10:20:00|2023-07-11 19:20:00|\n|  1|manish|11-07-2023|11:50|2023-07-11 11:50:00|2023-07-11 10:20:00|2023-07-11 19:20:00|\n|  1|manish|11-07-2023|19:20|2023-07-11 19:20:00|2023-07-11 10:20:00|2023-07-11 19:20:00|\n|  1|manish|12-07-2023|10:32|2023-07-12 10:32:00|2023-07-12 10:32:00|2023-07-12 16:23:00|\n|  1|manish|12-07-2023|12:20|2023-07-12 12:20:00|2023-07-12 10:32:00|2023-07-12 16:23:00|\n|  1|manish|12-07-2023|16:23|2023-07-12 16:23:00|2023-07-12 10:32:00|2023-07-12 16:23:00|\n|  2|rajesh|11-07-2023|11:20|2023-07-11 11:20:00|2023-07-11 11:20:00|2023-07-11 17:20:00|\n|  2|rajesh|11-07-2023|13:20|2023-07-11 13:20:00|2023-07-11 11:20:00|2023-07-11 17:20:00|\n|  2|rajesh|11-07-2023|17:20|2023-07-11 17:20:00|2023-07-11 11:20:00|2023-07-11 17:20:00|\n|  3|vikash|12-07-2023|09:12|2023-07-12 09:12:00|2023-07-12 09:12:00|2023-07-12 18:08:00|\n|  3|vikash|12-07-2023|18:08|2023-07-12 18:08:00|2023-07-12 09:12:00|2023-07-12 18:08:00|\n+---+------+----------+-----+-------------------+-------------------+-------------------+\n\n+---+------+----------+-------------------+-------------------+-----------------------------------+\n|id |name  |date      |entry time         |exit time          |time_diff                          |\n+---+------+----------+-------------------+-------------------+-----------------------------------+\n|1  |manish|11-07-2023|2023-07-11 10:20:00|2023-07-11 19:20:00|INTERVAL '0 09:00:00' DAY TO SECOND|\n|1  |manish|12-07-2023|2023-07-12 10:32:00|2023-07-12 16:23:00|INTERVAL '0 05:51:00' DAY TO SECOND|\n|2  |rajesh|11-07-2023|2023-07-11 11:20:00|2023-07-11 17:20:00|INTERVAL '0 06:00:00' DAY TO SECOND|\n|3  |vikash|12-07-2023|2023-07-12 09:12:00|2023-07-12 18:08:00|INTERVAL '0 08:56:00' DAY TO SECOND|\n+---+------+----------+-------------------+-------------------+-----------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# create data frame..\n",
    "emp_data = [(1,\"manish\",\"11-07-2023\",\"10:20\"),\n",
    "            (1,\"manish\",\"11-07-2023\",\"11:20\"),\n",
    "            (2,\"rajesh\",\"11-07-2023\",\"11:20\"),\n",
    "            (1,\"manish\",\"11-07-2023\",\"11:50\"),\n",
    "            (2,\"rajesh\",\"11-07-2023\",\"13:20\"),\n",
    "            (1,\"manish\",\"11-07-2023\",\"19:20\"),\n",
    "            (2,\"rajesh\",\"11-07-2023\",\"17:20\"),\n",
    "            (1,\"manish\",\"12-07-2023\",\"10:32\"),\n",
    "            (1,\"manish\",\"12-07-2023\",\"12:20\"),\n",
    "            (3,\"vikash\",\"12-07-2023\",\"09:12\"),\n",
    "            (1,\"manish\",\"12-07-2023\",\"16:23\"),\n",
    "            (3,\"vikash\",\"12-07-2023\",\"18:08\")]\n",
    "\n",
    "emp_schema = [\"id\", \"name\", \"date\", \"time\"]\n",
    "emp_df = spark.createDataFrame(data=emp_data, schema=emp_schema)\n",
    "emp_df.show()\n",
    "\n",
    "\n",
    "# use rowBetween to find the first entry and last exit of employee in office for tracking of employee moments.\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# combine the date and time string and convert into timestamp type..\n",
    "emp_df1 = emp_df.withColumn(\n",
    "                    'timestamp',\n",
    "                    to_timestamp(concat_ws(' ', 'date', 'time'), \"dd-MM-yyyy HH:mm\")\n",
    "                )\n",
    "emp_df1.show()\n",
    "emp_df1.printSchema()\n",
    "\n",
    "# use window function..\n",
    "window_func = Window.partitionBy('id', 'date').orderBy('date').rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "emp_df1 = emp_df1.withColumn(\n",
    "                    'entry time',\n",
    "                    first('timestamp').over(window_func)\n",
    "                ).withColumn(\n",
    "                    'exit time',\n",
    "                    last('timestamp').over(window_func)\n",
    "                )\n",
    "emp_df1.show()\n",
    "\n",
    "# find distinct records..\n",
    "emp_df1 = emp_df1.select('id', 'name', 'date', 'entry time', 'exit time').distinct()\n",
    "emp_df1.withColumn(\n",
    "            'time_diff',\n",
    "            (col('exit time') - col('entry time'))\n",
    "        ).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5527014-332f-48f1-9747-bd8a15b14413",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "join_and_window_opearations",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
