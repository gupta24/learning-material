{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b00cca7-fcd3-4dff-944c-772ff7b0ca6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=442497685451589#setting/sparkui/1127-123534-cn21uxln/driver-2411907344676498528\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=442497685451589#setting/sparkui/1127-123534-cn21uxln/driver-2411907344676498528\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f31e6c8-f426-4972-943f-32b5b484f655",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tables/Multi_line_correct.json</td><td>Multi_line_correct.json</td><td>310</td><td>1726033887000</td></tr><tr><td>dbfs:/FileStore/tables/Multi_line_incorrect.json</td><td>Multi_line_incorrect.json</td><td>304</td><td>1726033888000</td></tr><tr><td>dbfs:/FileStore/tables/badrecords/</td><td>badrecords/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/bucket_by_id/</td><td>bucket_by_id/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/corrupted_json.json</td><td>corrupted_json.json</td><td>218</td><td>1726033889000</td></tr><tr><td>dbfs:/FileStore/tables/csv_write/</td><td>csv_write/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/datafile_for_write.txt</td><td>datafile_for_write.txt</td><td>444</td><td>1726143143000</td></tr><tr><td>dbfs:/FileStore/tables/df_write_file.csv</td><td>df_write_file.csv</td><td>444</td><td>1726144455000</td></tr><tr><td>dbfs:/FileStore/tables/flight_data.csv</td><td>flight_data.csv</td><td>7080</td><td>1727954759000</td></tr><tr><td>dbfs:/FileStore/tables/line_delimited_json.json</td><td>line_delimited_json.json</td><td>219</td><td>1726033889000</td></tr><tr><td>dbfs:/FileStore/tables/multiline_nested_json.json</td><td>multiline_nested_json.json</td><td>610</td><td>1726048295000</td></tr><tr><td>dbfs:/FileStore/tables/multiline_zipcode-2.json</td><td>multiline_zipcode-2.json</td><td>239</td><td>1725625726000</td></tr><tr><td>dbfs:/FileStore/tables/nc.exe</td><td>nc.exe</td><td>38616</td><td>1729685136000</td></tr><tr><td>dbfs:/FileStore/tables/parquet_partition_data/</td><td>parquet_partition_data/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/part_r_00000_1a9822ba_b8fb_4d8e_844a_ea30d0801b9e_gz.parquet</td><td>part_r_00000_1a9822ba_b8fb_4d8e_844a_ea30d0801b9e_gz.parquet</td><td>3921</td><td>1726132059000</td></tr><tr><td>dbfs:/FileStore/tables/partition_by_address/</td><td>partition_by_address/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/partition_by_address_gender/</td><td>partition_by_address_gender/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/partition_by_id/</td><td>partition_by_id/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/resturant_json_data.json</td><td>resturant_json_data.json</td><td>669503</td><td>1727173988000</td></tr><tr><td>dbfs:/FileStore/tables/simple_file1.csv/</td><td>simple_file1.csv/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/FileStore/tables/simple_file2.csv</td><td>simple_file2.csv</td><td>139</td><td>1725879650000</td></tr><tr><td>dbfs:/FileStore/tables/simple_zipcodes-2.csv</td><td>simple_zipcodes-2.csv</td><td>590</td><td>1725626333000</td></tr><tr><td>dbfs:/FileStore/tables/single_file_json_with_extra_fields.json</td><td>single_file_json_with_extra_fields.json</td><td>232</td><td>1726033887000</td></tr><tr><td>dbfs:/FileStore/tables/structured-streaming/</td><td>structured-streaming/</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/tables/Multi_line_correct.json",
         "Multi_line_correct.json",
         310,
         1726033887000
        ],
        [
         "dbfs:/FileStore/tables/Multi_line_incorrect.json",
         "Multi_line_incorrect.json",
         304,
         1726033888000
        ],
        [
         "dbfs:/FileStore/tables/badrecords/",
         "badrecords/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/bucket_by_id/",
         "bucket_by_id/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/corrupted_json.json",
         "corrupted_json.json",
         218,
         1726033889000
        ],
        [
         "dbfs:/FileStore/tables/csv_write/",
         "csv_write/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/datafile_for_write.txt",
         "datafile_for_write.txt",
         444,
         1726143143000
        ],
        [
         "dbfs:/FileStore/tables/df_write_file.csv",
         "df_write_file.csv",
         444,
         1726144455000
        ],
        [
         "dbfs:/FileStore/tables/flight_data.csv",
         "flight_data.csv",
         7080,
         1727954759000
        ],
        [
         "dbfs:/FileStore/tables/line_delimited_json.json",
         "line_delimited_json.json",
         219,
         1726033889000
        ],
        [
         "dbfs:/FileStore/tables/multiline_nested_json.json",
         "multiline_nested_json.json",
         610,
         1726048295000
        ],
        [
         "dbfs:/FileStore/tables/multiline_zipcode-2.json",
         "multiline_zipcode-2.json",
         239,
         1725625726000
        ],
        [
         "dbfs:/FileStore/tables/nc.exe",
         "nc.exe",
         38616,
         1729685136000
        ],
        [
         "dbfs:/FileStore/tables/parquet_partition_data/",
         "parquet_partition_data/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/part_r_00000_1a9822ba_b8fb_4d8e_844a_ea30d0801b9e_gz.parquet",
         "part_r_00000_1a9822ba_b8fb_4d8e_844a_ea30d0801b9e_gz.parquet",
         3921,
         1726132059000
        ],
        [
         "dbfs:/FileStore/tables/partition_by_address/",
         "partition_by_address/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/partition_by_address_gender/",
         "partition_by_address_gender/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/partition_by_id/",
         "partition_by_id/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/resturant_json_data.json",
         "resturant_json_data.json",
         669503,
         1727173988000
        ],
        [
         "dbfs:/FileStore/tables/simple_file1.csv/",
         "simple_file1.csv/",
         0,
         0
        ],
        [
         "dbfs:/FileStore/tables/simple_file2.csv",
         "simple_file2.csv",
         139,
         1725879650000
        ],
        [
         "dbfs:/FileStore/tables/simple_zipcodes-2.csv",
         "simple_zipcodes-2.csv",
         590,
         1725626333000
        ],
        [
         "dbfs:/FileStore/tables/single_file_json_with_extra_fields.json",
         "single_file_json_with_extra_fields.json",
         232,
         1726033887000
        ],
        [
         "dbfs:/FileStore/tables/structured-streaming/",
         "structured-streaming/",
         0,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs\n",
    "ls FileStore/tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "443936f2-5317-41bc-a350-186ebfca4255",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "--------------------------- DATA EVOLUTION ------------------------------\n",
    "\n",
    "###### In data management, a schema defines the structure of your data, including the types, names, and organization of fields in a dataset. Schema evolution refers to the ability to handle changes in the schema over time without breaking compatibility with the data thatâ€™s already stored. In other words, schema evolution allows you to modify the structure of your dataset without needing to rewrite or discard existing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ac8e14c-3523-4541-a842-306b83578137",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+--------------------+--------------------+--------+\n| id|           name|               email|              region|currency|\n+---+---------------+--------------------+--------------------+--------+\n|  1|  Karyn Bentley| sit.amet@icloud.com|              Samsun|   35.33|\n|  2|   Maggie Short|hendrerit.donec@i...|               Viken|   31.51|\n|  3|Angelica Church|odio.sagittis@yah...|              Mersin|   30.26|\n|  4|  Sandra Alston|      nullam@aol.com|Brussels Hoofdste...|   26.77|\n|  5|   Ursula Stark|nulla.ante.iaculi...|        South Island|   41.09|\n+---+---------------+--------------------+--------------------+--------+\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-762089597122693>:17\u001B[0m\n",
       "\u001B[1;32m     14\u001B[0m df\u001B[38;5;241m.\u001B[39mshow()\n",
       "\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m#--------- Store The DataFrame Into Parquet Format\u001B[39;00m\n",
       "\u001B[0;32m---> 17\u001B[0m df\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mparquet(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/FileStore/tables/SchemaOperation\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\u001B[1;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStore parquet data successfully.....\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1655\u001B[0m, in \u001B[0;36mDataFrameWriter.parquet\u001B[0;34m(self, path, mode, partitionBy, compression)\u001B[0m\n",
       "\u001B[1;32m   1653\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpartitionBy(partitionBy)\n",
       "\u001B[1;32m   1654\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(compression\u001B[38;5;241m=\u001B[39mcompression)\n",
       "\u001B[0;32m-> 1655\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparquet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Path dbfs:/FileStore/tables/SchemaOperation already exists."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-762089597122693>:17\u001B[0m\n\u001B[1;32m     14\u001B[0m df\u001B[38;5;241m.\u001B[39mshow()\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m#--------- Store The DataFrame Into Parquet Format\u001B[39;00m\n\u001B[0;32m---> 17\u001B[0m df\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mparquet(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/FileStore/tables/SchemaOperation\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStore parquet data successfully.....\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1655\u001B[0m, in \u001B[0;36mDataFrameWriter.parquet\u001B[0;34m(self, path, mode, partitionBy, compression)\u001B[0m\n\u001B[1;32m   1653\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpartitionBy(partitionBy)\n\u001B[1;32m   1654\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(compression\u001B[38;5;241m=\u001B[39mcompression)\n\u001B[0;32m-> 1655\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparquet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Path dbfs:/FileStore/tables/SchemaOperation already exists.",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Path dbfs:/FileStore/tables/SchemaOperation already exists.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# When We Add, Remove, Change The Data Type Of Columns, Renaming Column Or Ordering The Columns\n",
    "# With Using The Parquet File That Is Called Data Evolution. And Such Of This Facility Provided By Parquet.\n",
    "\n",
    "#------- Creating The DataSet\n",
    "\n",
    "data = [[\"1\", \"Karyn Bentley\", \"sit.amet@icloud.com\", \"Samsun\", 35.33],\n",
    "        [\"2\", \"Maggie Short\", \"hendrerit.donec@icloud.com\", \"Viken\", 31.51],\n",
    "        [\"3\", \"Angelica Church\", \"odio.sagittis@yahoo.com\", \"Mersin\", 30.26],\n",
    "        [\"4\", \"Sandra Alston\", \"nullam@aol.com\", \"Brussels Hoofdstedelijk Gewest\", 26.77],\n",
    "        [\"5\", \"Ursula Stark\", \"nulla.ante.iaculis@protonmail.net\", \"South Island\", 41.09]]\n",
    "schema = \"id string, name string, email string, region string, currency float\"\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "\n",
    "#--------- Store The DataFrame Into Parquet Format\n",
    "df.write.parquet('/FileStore/tables/SchemaOperation')\n",
    "print(\"Store parquet data successfully.....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21c71e80-3e81-48fa-b997-f144b412967a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tables/SchemaOperation/_SUCCESS</td><td>_SUCCESS</td><td>0</td><td>1732546612000</td></tr><tr><td>dbfs:/FileStore/tables/SchemaOperation/_committed_2534771882733452823</td><td>_committed_2534771882733452823</td><td>624</td><td>1732546612000</td></tr><tr><td>dbfs:/FileStore/tables/SchemaOperation/_started_2534771882733452823</td><td>_started_2534771882733452823</td><td>0</td><td>1732546611000</td></tr><tr><td>dbfs:/FileStore/tables/SchemaOperation/part-00000-tid-2534771882733452823-eabc1c02-6b4e-43a4-96c1-2084b573eb90-992-1-c000.snappy.parquet</td><td>part-00000-tid-2534771882733452823-eabc1c02-6b4e-43a4-96c1-2084b573eb90-992-1-c000.snappy.parquet</td><td>762</td><td>1732546612000</td></tr><tr><td>dbfs:/FileStore/tables/SchemaOperation/part-00001-tid-2534771882733452823-eabc1c02-6b4e-43a4-96c1-2084b573eb90-993-1-c000.snappy.parquet</td><td>part-00001-tid-2534771882733452823-eabc1c02-6b4e-43a4-96c1-2084b573eb90-993-1-c000.snappy.parquet</td><td>1688</td><td>1732546612000</td></tr><tr><td>dbfs:/FileStore/tables/SchemaOperation/part-00003-tid-2534771882733452823-eabc1c02-6b4e-43a4-96c1-2084b573eb90-995-1-c000.snappy.parquet</td><td>part-00003-tid-2534771882733452823-eabc1c02-6b4e-43a4-96c1-2084b573eb90-995-1-c000.snappy.parquet</td><td>1723</td><td>1732546612000</td></tr><tr><td>dbfs:/FileStore/tables/SchemaOperation/part-00004-tid-2534771882733452823-eabc1c02-6b4e-43a4-96c1-2084b573eb90-996-1-c000.snappy.parquet</td><td>part-00004-tid-2534771882733452823-eabc1c02-6b4e-43a4-96c1-2084b573eb90-996-1-c000.snappy.parquet</td><td>1730</td><td>1732546612000</td></tr><tr><td>dbfs:/FileStore/tables/SchemaOperation/part-00006-tid-2534771882733452823-eabc1c02-6b4e-43a4-96c1-2084b573eb90-998-1-c000.snappy.parquet</td><td>part-00006-tid-2534771882733452823-eabc1c02-6b4e-43a4-96c1-2084b573eb90-998-1-c000.snappy.parquet</td><td>1824</td><td>1732546612000</td></tr><tr><td>dbfs:/FileStore/tables/SchemaOperation/part-00007-tid-2534771882733452823-eabc1c02-6b4e-43a4-96c1-2084b573eb90-999-1-c000.snappy.parquet</td><td>part-00007-tid-2534771882733452823-eabc1c02-6b4e-43a4-96c1-2084b573eb90-999-1-c000.snappy.parquet</td><td>1825</td><td>1732546612000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/tables/SchemaOperation/_SUCCESS",
         "_SUCCESS",
         0,
         1732546612000
        ],
        [
         "dbfs:/FileStore/tables/SchemaOperation/_committed_2534771882733452823",
         "_committed_2534771882733452823",
         624,
         1732546612000
        ],
        [
         "dbfs:/FileStore/tables/SchemaOperation/_started_2534771882733452823",
         "_started_2534771882733452823",
         0,
         1732546611000
        ],
        [
         "dbfs:/FileStore/tables/SchemaOperation/part-00000-tid-2534771882733452823-eabc1c02-6b4e-43a4-96c1-2084b573eb90-992-1-c000.snappy.parquet",
         "part-00000-tid-2534771882733452823-eabc1c02-6b4e-43a4-96c1-2084b573eb90-992-1-c000.snappy.parquet",
         762,
         1732546612000
        ],
        [
         "dbfs:/FileStore/tables/SchemaOperation/part-00001-tid-2534771882733452823-eabc1c02-6b4e-43a4-96c1-2084b573eb90-993-1-c000.snappy.parquet",
         "part-00001-tid-2534771882733452823-eabc1c02-6b4e-43a4-96c1-2084b573eb90-993-1-c000.snappy.parquet",
         1688,
         1732546612000
        ],
        [
         "dbfs:/FileStore/tables/SchemaOperation/part-00003-tid-2534771882733452823-eabc1c02-6b4e-43a4-96c1-2084b573eb90-995-1-c000.snappy.parquet",
         "part-00003-tid-2534771882733452823-eabc1c02-6b4e-43a4-96c1-2084b573eb90-995-1-c000.snappy.parquet",
         1723,
         1732546612000
        ],
        [
         "dbfs:/FileStore/tables/SchemaOperation/part-00004-tid-2534771882733452823-eabc1c02-6b4e-43a4-96c1-2084b573eb90-996-1-c000.snappy.parquet",
         "part-00004-tid-2534771882733452823-eabc1c02-6b4e-43a4-96c1-2084b573eb90-996-1-c000.snappy.parquet",
         1730,
         1732546612000
        ],
        [
         "dbfs:/FileStore/tables/SchemaOperation/part-00006-tid-2534771882733452823-eabc1c02-6b4e-43a4-96c1-2084b573eb90-998-1-c000.snappy.parquet",
         "part-00006-tid-2534771882733452823-eabc1c02-6b4e-43a4-96c1-2084b573eb90-998-1-c000.snappy.parquet",
         1824,
         1732546612000
        ],
        [
         "dbfs:/FileStore/tables/SchemaOperation/part-00007-tid-2534771882733452823-eabc1c02-6b4e-43a4-96c1-2084b573eb90-999-1-c000.snappy.parquet",
         "part-00007-tid-2534771882733452823-eabc1c02-6b4e-43a4-96c1-2084b573eb90-999-1-c000.snappy.parquet",
         1825,
         1732546612000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs\n",
    "ls FileStore/tables/SchemaOperation/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1920a9e-fa22-4a86-bd24-d0fd0b9a0f54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "----------------- ADDING AND REMOING THE FIELD IN PARQUET FILE ----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7c50eae-8c9a-4c52-b8dc-17bc54375c90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+--------------------+--------------------+--------+\n| id|           name|               email|              region|currency|\n+---+---------------+--------------------+--------------------+--------+\n|  5|   Ursula Stark|nulla.ante.iaculi...|        South Island|   41.09|\n|  4|  Sandra Alston|      nullam@aol.com|Brussels Hoofdste...|   26.77|\n|  3|Angelica Church|odio.sagittis@yah...|              Mersin|   30.26|\n|  2|   Maggie Short|hendrerit.donec@i...|               Viken|   31.51|\n|  1|  Karyn Bentley| sit.amet@icloud.com|              Samsun|   35.33|\n| 14|         djhfjh|               hdjjd|             djfdcia|    null|\n| 13|           djht|             adhdjht|              dkjdUd|    null|\n+---+---------------+--------------------+--------------------+--------+\n\n+---+------+-------+-------+-----+\n| id|  name|  email| region|phone|\n+---+------+-------+-------+-----+\n| 14|djhfjh|  hdjjd|djfdcia| 3838|\n| 13|  djht|adhdjht| dkjdUd|23349|\n+---+------+-------+-------+-----+\n\n+---+---------------+---------------------------------+------------------------------+--------+\n|id |name           |email                            |region                        |currency|\n+---+---------------+---------------------------------+------------------------------+--------+\n|5  |Ursula Stark   |nulla.ante.iaculis@protonmail.net|South Island                  |41.09   |\n|4  |Sandra Alston  |nullam@aol.com                   |Brussels Hoofdstedelijk Gewest|26.77   |\n|3  |Angelica Church|odio.sagittis@yahoo.com          |Mersin                        |30.26   |\n|2  |Maggie Short   |hendrerit.donec@icloud.com       |Viken                         |31.51   |\n|1  |Karyn Bentley  |sit.amet@icloud.com              |Samsun                        |35.33   |\n|14 |djhfjh         |hdjjd                            |djfdcia                       |null    |\n|14 |djhfjh         |hdjjd                            |djfdcia                       |null    |\n|13 |djht           |adhdjht                          |dkjdUd                        |null    |\n|13 |djht           |adhdjht                          |dkjdUd                        |null    |\n+---+---------------+---------------------------------+------------------------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "#------- Read The Parquet File and Add and Remove The New Column In Schema.\n",
    "parq_df = spark.read.parquet('/FileStore/tables/SchemaOperation')\n",
    "parq_df.show()\n",
    "\n",
    "#------- Create Another Data With Adding New Field Age And Remoing Field Region From Parquet Data\n",
    "data1 = [[\"14\", \"djhfjh\", \"hdjjd\", \"djfdcia\", 3838],\n",
    "         [\"13\",\"djht\", \"adhdjht\", \"dkjdUd\", 23349]]\n",
    "schema1 = \"id string, name string, email string, region string, phone int\"\n",
    "df1 = spark.createDataFrame(data1, schema1)\n",
    "df1.show()\n",
    "\n",
    "#------- Write The Data With New Schema Into Existing Parquet File\n",
    "df1.write.mode('append').option('mergeSchema', 'true').parquet('/FileStore/tables/SchemaOperation')\n",
    "\n",
    "#------- Read Again The Parquet File With Combine Data Schema\n",
    "parq_df = spark.read.parquet('/FileStore/tables/SchemaOperation')\n",
    "parq_df.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee2a4c40-e43d-47c9-8aa0-11842ce16e87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "------------- UPDATE THE DATA TYPE --------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c07322b-dcd7-491f-b48a-d72fe589a80e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update the parquet schema.............\n+---+---------------+--------------------+--------------------+--------+\n| id|           name|               email|              region|currency|\n+---+---------------+--------------------+--------------------+--------+\n|  5|   Ursula Stark|nulla.ante.iaculi...|        South Island|   41.09|\n|  4|  Sandra Alston|      nullam@aol.com|Brussels Hoofdste...|   26.77|\n|  3|Angelica Church|odio.sagittis@yah...|              Mersin|   30.26|\n|  2|   Maggie Short|hendrerit.donec@i...|               Viken|   31.51|\n|  1|  Karyn Bentley| sit.amet@icloud.com|              Samsun|   35.33|\n| 14|         djhfjh|               hdjjd|             djfdcia|    null|\n| 14|         djhfjh|               hdjjd|             djfdcia|    null|\n| 13|           djht|             adhdjht|              dkjdUd|    null|\n| 13|           djht|             adhdjht|              dkjdUd|    null|\n+---+---------------+--------------------+--------------------+--------+\n\nroot\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- email: string (nullable = true)\n |-- region: string (nullable = true)\n |-- currency: float (nullable = true)\n\nOverwrite update schmea into the parquet.............\nOverwrite the parquet schema.............\n+---+---------------+--------------------+--------------------+--------+\n| id|           name|               email|              region|currency|\n+---+---------------+--------------------+--------------------+--------+\n|  5|   Ursula Stark|nulla.ante.iaculi...|        South Island|   41.09|\n|  4|  Sandra Alston|      nullam@aol.com|Brussels Hoofdste...|   26.77|\n|  3|Angelica Church|odio.sagittis@yah...|              Mersin|   30.26|\n|  2|   Maggie Short|hendrerit.donec@i...|               Viken|   31.51|\n|  1|  Karyn Bentley| sit.amet@icloud.com|              Samsun|   35.33|\n| 14|         djhfjh|               hdjjd|             djfdcia|    null|\n| 13|           djht|             adhdjht|              dkjdUd|    null|\n| 14|         djhfjh|               hdjjd|             djfdcia|    null|\n| 13|           djht|             adhdjht|              dkjdUd|    null|\n+---+---------------+--------------------+--------------------+--------+\n\nroot\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- email: string (nullable = true)\n |-- region: string (nullable = true)\n |-- currency: float (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#------- Update The DataType Of Existing Parquet Schema.\n",
    "print(\"Update the parquet schema.............\")\n",
    "upd_parq_df = spark.read.parquet('/FileStore/tables/SchemaOperation').withColumn('id', col('id').cast('int'))\n",
    "upd_parq_df.show()\n",
    "upd_parq_df.printSchema()\n",
    "\n",
    "#parq_df1 = parq_df.select(col('id').cast('int'), col('name'), col('email'), col('region'), col('phone'))\n",
    "\n",
    "print(\"Overwrite update schmea into the parquet.............\")\n",
    "upd_parq_df.write.mode(\"overwrite\").option('overwriteSchema', 'true').parquet('/FileStore/tables/SchemaOperation')\n",
    "\n",
    "print(\"Overwrite the parquet schema.............\")\n",
    "upd_parq_df = spark.read.parquet('/FileStore/tables/SchemaOperation')\n",
    "upd_parq_df.show()\n",
    "upd_parq_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ddcb020-b796-4ea5-82f0-742d572ac31c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "-------------- RENAME COLUMN ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e93efc86-d5de-4096-b0da-69f91ea730ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+--------------------+-------------------+-----+\n|       United States|            Romania|    1|\n|       United States|            Ireland|  264|\n|       United States|              India|   69|\n|               Egypt|      United States|   24|\n|   Equatorial Guinea|      United States|    1|\n|       United States|          Singapore|   25|\n|       United States|            Grenada|   54|\n|          Costa Rica|      United States|  477|\n|             Senegal|      United States|   29|\n|       United States|   Marshall Islands|   44|\n|              Guyana|      United States|   17|\n|       United States|       Sint Maarten|   53|\n|               Malta|      United States|    1|\n|             Bolivia|      United States|   46|\n|            Anguilla|      United States|   21|\n|Turks and Caicos ...|      United States|  136|\n|       United States|        Afghanistan|    2|\n|Saint Vincent and...|      United States|    1|\n|               Italy|      United States|  390|\n|       United States|             Russia|  156|\n+--------------------+-------------------+-----+\nonly showing top 20 rows\n\nroot\n |-- DEST_COUNTRY_NAME: string (nullable = true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n |-- count: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "parq_data = spark.read.parquet('/FileStore/tables/part_r_00000_1a9822ba_b8fb_4d8e_844a_ea30d0801b9e_gz.parquet')\n",
    "parq_data.show()\n",
    "schema = parq_data.printSchema()\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77938f3a-501c-4f63-827f-a815c136d05a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('email', StringType(), True), StructField('region', StringType(), True), StructField('currency', FloatType(), True)])\n+---+---------------+--------------------+--------------------+--------+\n| id|           name|               email|              region|currency|\n+---+---------------+--------------------+--------------------+--------+\n|  5|   Ursula Stark|nulla.ante.iaculi...|        South Island|   41.09|\n|  4|  Sandra Alston|      nullam@aol.com|Brussels Hoofdste...|   26.77|\n|  3|Angelica Church|odio.sagittis@yah...|              Mersin|   30.26|\n|  2|   Maggie Short|hendrerit.donec@i...|               Viken|   31.51|\n|  1|  Karyn Bentley| sit.amet@icloud.com|              Samsun|   35.33|\n+---+---------------+--------------------+--------------------+--------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_schema = spark.read.parquet('/FileStore/tables/SchemaOperation')\n",
    "drop_column = ['id', 'name', 'age']\n",
    "schema1 = df_schema.drop(*drop_column).schema\n",
    "print(schema1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1871eda-4a3f-4afc-92ce-22f7063ff0c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "------------ difference of  InferSchema, MergeSchema, and OverwriteSchema -----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a63c22eb-485e-4866-b055-47097f3d221a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3065592321701707,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "parquet file schema operations",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
