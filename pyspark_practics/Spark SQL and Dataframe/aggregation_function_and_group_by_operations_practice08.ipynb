{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d19d60e-5077-445c-8916-3720964d7ef5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=442497685451589#setting/sparkui/0919-114440-jn677jts/driver-7633581835821463912\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=442497685451589#setting/sparkui/0919-114440-jn677jts/driver-7633581835821463912\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "464d6f55-4a95-4452-a54c-21575736b6ce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-------------- Create Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21c8c1af-8807-4f5a-bded-cfb6c854994c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----+------+-------+-----------+\n|  id|   name| age|   sal|country|       dept|\n+----+-------+----+------+-------+-----------+\n|   1| manish|  26| 20000|  india|         IT|\n|   2|  rahul|null| 40000|germany|engineering|\n|   3|  pawan|  12| 60000|  india|      sales|\n|   4|roshini|  44|  null|     uk|engineering|\n|   5|raushan|  35| 70000|  india|      sales|\n|   6|   null|  29|200000|     uk|         IT|\n|   7|   adam|  37| 65000|     us|         IT|\n|   8|  chris|  16| 40000|     us|      sales|\n|null|   null|null|  null|   null|       null|\n|   7|   adam|  37| 65000|     us|         IT|\n+----+-------+----+------+-------+-----------+\n\n+---+-------+------+---------+\n| id|   name|salary|     dept|\n+---+-------+------+---------+\n|  1| manish| 50000|       IT|\n|  2| vikash| 60000|    sales|\n|  3|raushan| 70000|marketing|\n|  4| mukesh| 80000|       IT|\n|  5| pritam| 90000|    sales|\n|  6| nikita| 45000|marketing|\n|  7| ragini| 55000|marketing|\n|  8| rakesh|100000|       IT|\n|  9| aditya| 65000|       IT|\n| 10|  rahul| 50000|marketing|\n+---+-------+------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "emp_data = [\n",
    "    (1,'manish',26,20000,'india','IT'),\n",
    "    (2,'rahul',None,40000,'germany','engineering'),\n",
    "    (3,'pawan',12,60000,'india','sales'),\n",
    "    (4,'roshini',44,None,'uk','engineering'),\n",
    "    (5,'raushan',35,70000,'india','sales'),\n",
    "    (6,None,29,200000,'uk','IT'),\n",
    "    (7,'adam',37,65000,'us','IT'),\n",
    "    (8,'chris',16,40000,'us','sales'),\n",
    "    (None,None,None,None,None,None),\n",
    "    (7,'adam',37,65000,'us','IT')\n",
    "]\n",
    "schema = ['id', 'name', 'age', 'sal', 'country', 'dept']\n",
    "df = spark.createDataFrame(emp_data, schema)\n",
    "df.show()\n",
    "\n",
    "# data for group by statement\n",
    "group_by_data = [(1,'manish',50000,'IT'),\n",
    "                (2,'vikash',60000,'sales'),\n",
    "                (3,'raushan',70000,'marketing'),\n",
    "                (4,'mukesh',80000,'IT'),\n",
    "                (5,'pritam',90000,'sales'),\n",
    "                (6,'nikita',45000,'marketing'),\n",
    "                (7,'ragini',55000,'marketing'),\n",
    "                (8,'rakesh',100000,'IT'),\n",
    "                (9,'aditya',65000,'IT'),\n",
    "                (10,'rahul',50000,'marketing')]\n",
    "group_by_schema = ['id', 'name', 'salary', 'dept']\n",
    "df1 = spark.createDataFrame(group_by_data, group_by_schema)\n",
    "df1.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6bc4795-7060-45fb-b67f-f5c5af2f56ea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-------------- Find Count On Single And Munltiple Column As Action And Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e9d64ce-a89f-4131-8f45-bcd78f56e7e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action count is : 10\n+-----------+\n|count(name)|\n+-----------+\n|          8|\n+-----------+\n\ntransformation count on one column is : None\n+--------+\n|count(1)|\n+--------+\n|      10|\n+--------+\n\ntransformation count on entire column is : None\n"
     ]
    }
   ],
   "source": [
    "## use count as an action\n",
    "# get count on entire columns of table\n",
    "cnt = df.count()\n",
    "print(\"action count is :\", cnt)\n",
    "\n",
    "## use count as transformers\n",
    "# get count on single columns which will ignore the null value in count\n",
    "cnt = df.select(count(\"name\")).show()\n",
    "print(\"transformation count on one column is :\", cnt)\n",
    "# get count on entire column then not ignore null value in count\n",
    "cnt = df.select(count(\"*\")).show()\n",
    "print(\"transformation count on entire column is :\", cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d2bf604-fb1b-4804-a224-d36aa85d1c10",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "--------------- Use Aggregation Function Like SUM, MIN, MAX, AVG, COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ebb5ebc-478e-4c16-b5fb-2e65f925789a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+\n|sal_sum|min_sal|max_sal|\n+-------+-------+-------+\n| 560000|  20000| 200000|\n+-------+-------+-------+\n\n+-------+---------+-----------+\n|sal_sum|sal_count|sal_average|\n+-------+---------+-----------+\n| 560000|        8|      70000|\n+-------+---------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# get sum, min, max\n",
    "df.select(sum(\"sal\").alias(\"sal_sum\"), min(\"sal\").alias(\"min_sal\"), max(\"sal\").alias(\"max_sal\")).show()\n",
    "\n",
    "# get the sum, count, avg of salary column\n",
    "df.select(sum(\"sal\").alias(\"sal_sum\"), count(\"sal\").alias(\"sal_count\"), avg(\"sal\").alias(\"sal_average\").cast('int')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43df4725-5fd6-4a06-ab73-02eb1bb936c8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "--------------- Work With Group By Statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d4b0284-0f12-458c-862e-f072eeef5f06",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n|total salary given to employees|\n+-------------------------------+\n|                         665000|\n+-------------------------------+\n\n+---------+------------+\n|     dept|total_salary|\n+---------+------------+\n|       IT|      295000|\n|    sales|      150000|\n|marketing|      220000|\n+---------+------------+\n\n+---+-------+------+---------+------------+\n| id|   name|salary|     dept|total_salary|\n+---+-------+------+---------+------------+\n|  1| manish| 50000|       IT|      295000|\n|  2| vikash| 60000|    sales|      150000|\n|  3|raushan| 70000|marketing|      220000|\n|  5| pritam| 90000|    sales|      150000|\n|  4| mukesh| 80000|       IT|      295000|\n|  6| nikita| 45000|marketing|      220000|\n|  7| ragini| 55000|marketing|      220000|\n|  8| rakesh|100000|       IT|      295000|\n| 10|  rahul| 50000|marketing|      220000|\n|  9| aditya| 65000|       IT|      295000|\n+---+-------+------+---------+------------+\n\n+---+-------+------+---------+------------+\n| id|   name|salary|     dept|total_salary|\n+---+-------+------+---------+------------+\n|  1| manish| 50000|       IT|      295000|\n|  4| mukesh| 80000|       IT|      295000|\n|  8| rakesh|100000|       IT|      295000|\n|  9| aditya| 65000|       IT|      295000|\n|  3|raushan| 70000|marketing|      220000|\n|  6| nikita| 45000|marketing|      220000|\n|  7| ragini| 55000|marketing|      220000|\n| 10|  rahul| 50000|marketing|      220000|\n|  2| vikash| 60000|    sales|      150000|\n|  5| pritam| 90000|    sales|      150000|\n+---+-------+------+---------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "\n",
    "\n",
    "# find the total salary given to employe..\n",
    "df1.select(sum('salary').alias(\"total salary given to employees\")).show()\n",
    "\n",
    "# find the total salary per department wise\n",
    "df1.groupBy('dept').agg(sum(\"salary\").alias(\"total_salary\")).show()\n",
    "\n",
    "## find the total salary per department wise with each column records using join or window function\n",
    "# using joins\n",
    "df2 = df1.groupBy('dept').agg(sum(\"salary\").alias(\"total_salary\"))\n",
    "df2.alias('group_df1').join(\n",
    "  df1.alias('actual_df1'),\n",
    "  (col('group_df1.dept') == col('actual_df1.dept'))\n",
    ").select(\n",
    "  col('actual_df1.id'),\n",
    "  col('actual_df1.name'),\n",
    "  col('actual_df1.salary'),\n",
    "  col('actual_df1.dept'),\n",
    "  col('group_df1.total_salary')\n",
    ").show()\n",
    "\n",
    "# using window function.\n",
    "# df2 = df1.alias('df2')\n",
    "# df2.select(col('id'), col('name'), col('salary'), col('dept'),\n",
    "#           sum(col('salary')).over(Window.partitionBy(col('dept'))).alias('total_salary')).show()\n",
    "\n",
    "df1.select(col('id'), col('name'), col('salary'), col('dept')).withColumn(\n",
    "  'total_salary', sum(col('salary')).over(Window.partitionBy(col('dept')))).show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "aggregation_function_and_group_by_operations",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
