{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "128bbd26-f08e-4f2c-9144-de1b6e44397e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=442497685451589#setting/sparkui/1023-120505-pabdo7zb/driver-8433220076017631464\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=442497685451589#setting/sparkui/1023-120505-pabdo7zb/driver-8433220076017631464\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65fea49e-0623-4eb1-8520-69116875b0be",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-------------- Create Spark Streaming Using RDD (low lavel languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7d24079-6b62-40c9-9941-3a27eeda130a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#--------------- First Run This SparkStreamin -------------\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# When we create SparkSession and run then internally default SparkContext will create with eight working thread. Where `spark` use for work with spark sql/dataframe and `sc` use for work with spark context.\n",
    "# Create a local StreamingContext with batch interval of 1 second\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# Create a DStream that will connect to hostname:port, like localhost:9999\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "# Split each line into words\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# Count each word in each batch\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Print the first ten elements of each RDD generated in this DStream to the console\n",
    "wordCounts.pprint()\n",
    "\n",
    "ssc.start()             # Start the computation\n",
    "ssc.awaitTermination()  # Wait for the computation to terminate\n",
    "\n",
    "\n",
    "\n",
    "#--------------- Second Run Natcat As Data Server For Passing Data -------------\n",
    "# If we have already downloaded and built Spark, you can run this example as follows. You will need to run Netcat (a small utility found in most Unix-like systems) as a data server by using\n",
    "\n",
    "$ nc -lk 9999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6871ca93-3af7-4720-bfd3-ab2882d19dec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "------------------ Create Spark Streaming Application Using Spark Streaming API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f947c261-3b54-468a-be3e-af548563b956",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------ This Is An Example of Reading and Writing Static Data ---------------------\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# first upload one file on this path after run this application upload again and again new one file.\n",
    "inputPath = \"/FileStore/tables/structured-streaming/events\"\n",
    "\n",
    "# Let's first define the schema before reading the data\n",
    "jsonSchema = StructType([ StructField(\"time\", TimestampType(), True), StructField(\"action\", StringType(), True) ])\n",
    "\n",
    "# Using readStream instead of read on streaming data\n",
    "streamingInputDF = spark.readStream\\\n",
    "                        .schema(jsonSchema)\\\n",
    "                        .option(\"maxFilesPerTrigger\", 1)\\\n",
    "                        .json(inputPath)\n",
    "\n",
    " \n",
    "# Treat a sequence of files as a stream by picking one file at a time\n",
    "# groupBy the data\n",
    "streamingCountsDF = streamingInputDF.groupBy(streamingInputDF.action, window(streamingInputDF.time, \"1 hour\")).count()\n",
    "\n",
    "\n",
    "# We start a streaming computation by defining a sink and starting it. In our case, to query the counts interactively, set the completeset of 1 hour counts to be in an in-memory table.\n",
    "query = (streamingCountsDF.writeStream\n",
    "                        .format(\"memory\")              # memory = store in-memory table (for testing only)\n",
    "                        .queryName(\"counts\")           # counts = name of the in-memory table\n",
    "                        .outputMode(\"complete\")        # complete = all the counts should be in the table\n",
    "                        .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c260d856-5cfb-42f3-801f-3ead801f69c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tables/structured-streaming/events/file0.json</td><td>file0.json</td><td>479</td><td>1729752069000</td></tr><tr><td>dbfs:/FileStore/tables/structured-streaming/events/file1.json</td><td>file1.json</td><td>489</td><td>1729753135000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/tables/structured-streaming/events/file0.json",
         "file0.json",
         479,
         1729752069000
        ],
        [
         "dbfs:/FileStore/tables/structured-streaming/events/file1.json",
         "file1.json",
         489,
         1729753135000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs\n",
    "ls FileStore/tables/structured-streaming/events/"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 981456244698581,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark Streaming Demo",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
